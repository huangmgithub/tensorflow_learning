{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理解语言的 Transformer 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 模型的核心思想是自注意力机制（self-attention）——能注意输入序列的不同位置以计算该序列的表示的能力。Transformer 创建了多层自注意力层（self-attetion layers）组成的堆栈，下文的按比缩放的点积注意力（Scaled dot product attention）和多头注意力（Multi-head attention）部分对此进行了说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个 transformer 模型用自注意力层而非 RNNs 或 CNNs 来处理变长的输入。这种通用架构有一系列的优势：\n",
    "+ 它不对数据间的时间/空间关系做任何假设。这是处理一组对象（objects）的理想选择（例如，星际争霸单位（StarCraft units））。\n",
    "+ 远距离项可以影响彼此的输出，而无需经过许多 RNN 步骤或卷积层（例如，参见场景记忆 Transformer（Scene Memory Transformer））\n",
    "+ 它能学习长距离的依赖。在许多序列任务中，这是一项挑战。  \n",
    "\n",
    "\n",
    "该架构的缺点是：\n",
    "+ 对于时间序列，一个单位时间的输出是从整个历史记录计算的，而非仅从输入和当前的隐含状态计算得到。这可能效率较低。\n",
    "+ 如果输入确实有时间/空间的关系，像文本，则必须加入一些位置编码，否则模型将有效地看到一堆单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此 notebook 中训练完模型后，您将能输入葡萄牙语句子，得到其英文翻译。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![translation](https://tensorflow.google.cn/images/tutorials/transformer/attention_map_portuguese.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# try:\n",
    "#     !pip install -q tf-nightly\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置输入流水线(input pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 TFDS 来导入 葡萄牙语-英语翻译数据集，该数据集来自于 TED 演讲开放翻译项目.\n",
    "\n",
    "该数据集包含来约 50000 条训练样本，1100 条验证样本，以及 2000 条测试样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load(\n",
    "    \"ted_hrlr_translate/pt_to_en\",\n",
    "    with_info = True,\n",
    "    as_supervised = True\n",
    ")\n",
    "train_examples, val_examples = examples[\"train\"], examples[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从训练数据集创建自定义子词分词器（subwords tokenizer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799]\n",
      "The original string :Transformer is awesome\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"Transformer is awesome\"\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print(\"Tokenized string is {}\".format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print(\"The original string :{}\".format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果单词不在词典中，则分词器（tokenizer）通过将单词分解为子词来对字符串进行编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8087"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7915 ----> T\n",
      "1248 ----> ran\n",
      "7946 ----> s\n",
      "7194 ----> former \n",
      "13 ----> is \n",
      "2799 ----> awesome\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print(\"{} ----> {}\".format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将开始和结束标记（token）添加到输入和目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    \"\"\"将开始和结束token添加进输入和目标输出中\"\"\"\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "            lang1.numpy()) + [tokenizer_pt.vocab_size+ 1]\n",
    "    \n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "            lang2.numpy()) + [tokenizer_en.vocab_size + 1]\n",
    "    \n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note：为了使本示例较小且相对较快，删除长度大于40个标记的样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.logical_and 相当于逻辑与\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                         tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".map() 内部的操作以图模式（graph mode）运行，.map() 接收一个不具有 numpy 属性的图张量（graph tensor）。该分词器（tokenizer）需要将一个字符串或 Unicode 符号，编码成整数。因此，您需要在 tf.py_function 内部运行编码过程，tf.py_function 接收一个 eager 张量，该 eager 张量有一个包含字符串值的 numpy 属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.py_function(func, inp, Tout)\n",
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "    \n",
    "    return result_pt, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# 将数据集缓存到内存中以加快读取速度\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=207703, shape=(64, 38), dtype=int64, numpy=\n",
       " array([[8214,  342, 3032, ...,    0,    0,    0],\n",
       "        [8214,   95,  198, ...,    0,    0,    0],\n",
       "        [8214, 4479, 7990, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,  584,   12, ...,    0,    0,    0],\n",
       "        [8214,   59, 1548, ...,    0,    0,    0],\n",
       "        [8214,  118,   34, ...,    0,    0,    0]], dtype=int64)>,\n",
       " <tf.Tensor: id=207704, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8087,   98,   25, ...,    0,    0,    0],\n",
       "        [8087,   12,   20, ...,    0,    0,    0],\n",
       "        [8087,   12, 5453, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,   18, 2059, ...,    0,    0,    0],\n",
       "        [8087,   16, 1436, ...,    0,    0,    0],\n",
       "        [8087,   15,   57, ...,    0,    0,    0]], dtype=int64)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为该模型并不包括任何的循环（recurrence）或卷积，所以模型添加了位置编码，为模型提供一些关于单词在句子中相对位置的信息。\n",
    "\n",
    "位置编码向量被加到嵌入（embedding）向量中。嵌入表示一个 d 维空间的标记，在 d 维空间中有着相似含义的标记会离彼此更近。但是，嵌入并没有对在一句话中的词的相对位置进行编码。因此，当加上位置编码后，词将基于它们含义的相似度以及它们在句子中的位置，在 d 维空间中离彼此更近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参看 位置编码 的 notebook 了解更多信息。计算位置编码的公式如下：\n",
    "\n",
    "$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$\n",
    "$$ PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angles_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angles_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis, :],\n",
    "                           d_model)\n",
    "    # 将sin应用于数组中的偶数索引（indices），2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # 将cos应用于数组中的奇数索引， 2i + 1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3gc1fm273dmd6VV77Jsyw1344oxNqaZ3g0kEFooIZBGAmmEFPJLT0i+EEgCIUAIkAKhBLAJzWDAYJox7jZucpesXlfbZuZ8f+ysvJIla2VLxrLPfV3H02fPyquzo+c97/OKUgqNRqPRHBkYn3YHNBqNRnPw0IO+RqPRHEHoQV+j0WiOIPSgr9FoNEcQetDXaDSaIwg96Gs0Gs0RRJ8O+iKyVURWichyEfnI3ZcnIgtEZKO7zO3LPmg0Gs2niYg8LCJVIrK6i+MiIn8UkU0islJEpiUcO1tE1rvHbu+N/hyMJ/05SqkpSqnp7vbtwOtKqVHA6+62RqPRHK48Apy9j+PnAKPcdhPwFwARMYF73ePjgStEZPyBdubTkHfmAo+6648CF30KfdBoNJqDglJqEVC3j1PmAo+pGO8DOSJSAswANimlypRSEeAJ99wDwnOgN+gGBbwqIgr4q1LqAaBYKVUBoJSqEJGizi4UkZuIfeuRnuY/Jq3VpnTKOJat38mUsUPYsWwNQycMZ/n2RtJzsxncXEF9Q5gBUydQ0xoho3wbtU1hBo8ZzPpmD631tRQOLGaQaqR8SzWphlAwdhg71mwhzTTIG1NKecRHdWUtynHIzM/jqHw/4R1l1NUGsRW0lAwl1NSIUoqUjCyK89LITwGrejeBqmaaLQeANNMgPSeFUGOYFtvBVuATIT3FJDXHjzc3Fyc1k+aITX0gQmvQIjvTR06qlzSvgREN4gSaiDS3Eg1EiEQcwo7CVgoHKJ16NIYVQoVbsYNBrGAYK2Rhh20ijoPl0HauAgZNmYDlKMK2Q8RWRCyHiGUTsRwcW8Wa46AcmzHeZkyvB8NjgseDeLyI6QXDRBlmbIngKFi5YUf8fwtEEHGX8W3D2LNtGKRlpKKUwnEUSgEqtlQqvg0q9g++VA8iIAix2wgCGCK4LxM7JlBZ1QDxzPL4jeL/dsw4V4oRwwbEP2PInncQexvuVnz7k007k/6wTxhVuufz28U5knBg1frtSd970pghXd+0w2uu+CT5+04ZOyTpcwGW9+jeQ3tw32096seUcZ3fe/m6bahgbY1SqrBHN+yAkTVYYYW6PU8Fa9cAiSc+4I5zPWEQsCNhe6e7r7P9x/Xw3nvR14P+bKVUuTuwLxCRT5K90P3BPQBwzKQJaubqAHe9tZDMk7/Donfu5Tvp47j36QfJv/kVjv/sufx64c95dv5Gvrd4MX9bVsHMn3+Rf71axp1/+zUnv5XP0qf+xeU//ia/jrzATz//IKMzfFz39IN8c8I1HJuTyhX/voc7dg7mL3/4N9FQgBOvuYynrzqaLbd8nn/9cxWNUYd3b/wT615/GScaYdjxZ/HtKyZzzQiTmvt/yQd/XsQb1a0ATMtO5bgLRrHx5TIW17bSGHUYmOJh1rBsxlw0iYGf/SyBsafy1rZGnvhoBytXVnLuycO5YMIApg5II618Ba0fvMqut5ZTvmQX27Y3sbU1Sl3EJuIo7lq8mJSajVgbl9GydhU1KzdTu76G+rIGdrVEqA7b1Edtgu4Xzs/efJuaoM22hiDbG0NsrQmwrTZAeW0rgaYwrY1hQq0Rws0NzCt5k/QBefiLcvHkFWLmD8DMLYL0HJyUTBx/DlEzhdaoQ+mptyKG2dZMrw/D48PweDE8PswUP6bH17Y+7YRRBCM24bCFFXGwojZW1Ma2HKyog2M52LaDbTkMGVOAx2Pg8xik+Ux8HgOfx12aBinuMZ/H4I9/fBZl2yhnTwNQ7hdZbD22dBybPzx4O6aA1zQwBEwRDBFMI/alkrg986K91cf4vTry/Ct3AbR9OcGeQT7+J7W4OwyBoXO+nuyvAwve+jNGwqDf2fgfP1504s1J3/etd+7t8lhnr5E3+2tJ3/udxfclfW7O8V9N+lyAxV3cO3vWV4ku/3vPvkE6wwrhGXNht6dFl/89lCBd7y+d/ajVPvYfEH066Culyt1llYg8S+zPlUoRKXGf8kuAqr7sg0aj0fQYEcQwD9ar7QRKE7YHA+WAr4v9B0Sfafoiki4imfF14ExgNTAPuNY97Vrg+b7qg0aj0ewf4v7Vuu/WS8wDrnFn8cwEGl0JfAkwSkSGi4gPuNw994Doyyf9YuBZ989ZD/BvpdTLIrIEeFJEbgC2A5f2YR80Go2m5/Tik76IPA6cAhSIyE7g/wAvgFLqfuBF4FxgE9AKXO8es0TkZuAVwAQeVkqtOdD+9Nmgr5QqAyZ3sr8WOK0n91pbFeHPpwzl6O+/xayrr+H9Y0/isolFXPZu7Jt23gW53PLVdfzol+dxzl8+4LVTgnz31TKuOH04r+efzMoXf8OQWedz51kjeH3M4wRtxRlfmsUHqeMxBU68YQYbimfy9AOv01pbztDjL+C200fjLHiIFfM2UB22mZaTypOrPyEaaCRvxGSmTi3hzJH5OB/+m60L1rCqMUzEUZT6vYwYmcugk6bw/JPraIw6ZHgMhqd7KZpYRMH0CaghE9neFGHpjga27GyiqaaeiYOmMCQ7hZTm3UTK1tCwYQcNW+ppqGihOmzTYjlEnJic5wnUoGp2YVVuJ7CrhtaqFlprgjSGLFosh4AdO9d21b+WqENDKEp9MEp9a4TaQITalgjhoEUkaBEJW0RDrdiRIL7MNLzpfsz0DIy0TIzUdMTnx/GkonxpKE8KEUsRsfdIi2KYiBnX9g3EMDG8PgxX6zc8PsQwiVgOlhXT7G071pQTCyQrR+Go2FIphRiCaQg+j4FpCKbhLkXc7T0tUc9v+5w5TpefJ1P2aO770vMN2VtS7UrPb/tZJPeR7jFGNzfu7nhP6av30V8QQMzeGfSVUld0c1wBnQZLlFIvEvtS6DX6OpCr0Wg0/Q8RjIOn6R9U9KCv0Wg0nXAQA7kHFT3oazQaTUcO7uydg4oe9DUajaYDgmB4vJ92N/qEfuGyGW5uwPfo8+z86DUWnmfy7LpqZn6wiBfufYhf/PwGXjvpSo7N9VNz3a/44IknWXDx9yjweZj28F+49d73ULbNHTccS82dt/LiribOKc2i5Fs/5XtPreSsoTkMvuUH/OCFtez6+A3SC0s59/SRzExrYM0DL7CkPkS212DarEE0bF+HNz2bQePHcPn0UgYFtrDrpYVsWF1NZdjCbwrjs3wMPn4Y6cedSmXYAqA4xUPpsBwGTB9J6sRZ1PvyWV7RzMfb6qmraCZQtZ3xhRkMSFXI7o2Etm6mYVM5TTubqA7bNFmxRCsAnyGYzVVuELeawO5aWioDtNYFaYw6bQFfOyETtSXiUBe0qAtFqWoKU9sSJhSMEglGiYStWIJUOIgVCeLLSsOblYaRnuW2TByfH8frR3lSiCqIOIqIo/YkZplmW9A2HsSVxCCuGVtGrHjwlljA1lHYthPL0HVUW3KWclRbENeTELD1mbFkrHhiVnx/Iu2DuXsnZoEbsDWk14OfcZJJzDoQjvQg60HBfdLvrvVH9JO+RqPRdEJ/HdS7Qw/6Go1G0xGRXpuyeaihB32NRqPpgHD4Pun3C02/dEgJp13//7jrnu9y1/Qb+O53T+bYHy6gZOrp3FD5HM+V1XPlvJ9yyS8WkpY/kOe3NXLtd0/hJysctrwzj0nnXcjVedXMv+dt8nwmJ/36Uh7doljz+tvM/r+5zK/L4sMFy7AjQUYcN4tbThxGw+N/5v3FO2mxHGbm+Rn3+Tk4VoT8kdM4bUYpc4ZlE1z0LFte28yGlgi2gmFpPkqnDaBkzkysoccQtBV5PpORGV5KppWQPWUK0ZIJbKoP8dG2esp3NNJcVUGkpZ7SLC9m/Xai2z6hfsMOGrc1UVcbbEvMiudC+QzBqdpOpGInLbuqaa5oobU2SF3EpjFqE3L19vj5pkB9MEpta4S6lgh1gQgN8cSssE00bGEFW7AjQZxoBF9WOmZ6Zpuer3zpKG8aeFNxPCmE3MSsiL1H0zdc7T5utJa4L67rG4bEkrISErNsK6bvx7X8Nm3fUe00+7jRmmlIe43f3deTxKw43Rmtxd08E+lJYlZXen5foBOz+gAxMD2+blt/RD/pazQaTUfk8H3S14O+RqPRdEDQ8/Q1Go3miOJwHfT7haafVb+LjOLhXPDSLwFYee2dbHzjWRb+5lzuueo+rjlpCPdY09j27ny+9e1LOas4Hc+37uaBB14ia/BoHrlxBsu+9l1WNIaYe+owAud+k98/voKWyq3w2e/xq2dWUbvpY/JHTuMrF4xjyK73WPHQO6xrDlPq93L0xePwnX4N6YWlDJ9YypXTBuHf+Dabn3+PldsbqYvY5PlMxg5Ip/SU8fimzmFTk8JnCKV+LwMnFjHguPF4xs9kV9jk44omVmyto66yhWD9biKBRnJUAGfHJzRv2EzDpkqadjaxOxSfox8T6H2GkOExsCq20Ly9ksDuBgKVAZobwzRGHUKOImjvMWaLX1PTGqG2NdI2Rz8ctAgHo0TDFtFQCDsSm6NvR4J4M9KRtCyMtEzEn4ny+VHeFByvn7DltOn5ccO1jkZr8e02Pd+ds2+YBo7tVuqyYgVTlFLYltPOaM1xFI4VadPvfR6zS6O1jvP0Y9q+07aeuHQS9PjEa3rLaC1OZ9e2Px5b7q/E3/Gyvso1OOLR8/Q1Go3mSELLOxqNRnPEICIY3v45O6c79KCv0Wg0HdGGaxqNRnNkoQf9T5HdlS1seuBKvpfxc+5d8wgFt/6FOTfeQODrnyNgO0x76SXOu+i3HHXKRdxeUo791B3M+csHNGxdzU0/upUhi+7np69t4djcVKbe9ROum7+Ore++QvaQcfzqjS1sfHsRHn8GU+ZM5uqjC9h86zd5p6weU4Tjx+Qx9OrLWB7MZMCEY/j8icMZ72+lav6zlC3azo5gFJ8hjM7wMWT2YHJPPIWGnKN4Z201BT6TEUVplEwfRvqUWQTzRrB6ayPvbqyhZlczgerthJvrUY6Np6aM1rI11G/YQcO2RiqbI9RH9wRxTYEMj0GWx6B1Z3ksMas8VjGrLhJL4EqsrgWxIG4skBuluilMXSBMcyBCOBQlErSIhq02ozUnGsGxokh6FkZmDpKeFQvielJQ3jQsDCK24zZFa9RuS8JKDGwZbtJKYhDX9BiYHgPbUm3JWY6jsC3VZrwWT8yKJ1p1NFXrmJiVmKC1d3JW10FcZdvtErO6QgSMHqYpHQ5GazouvAfjMI2S94vZOxqNRnMwERHE6L4lea+zRWS9iGwSkds7Of5dEVnuttUiYotInntsq4isco991BvvrV886Ws0Gs3BxjQP/JlYREzgXuAMYCewRETmKaXWxs9RSv0O+J17/gXAN5VSdQm3maOUqjngzrjoJ32NRqPpiNBbT/ozgE1KqTKlVAR4Api7j/OvAB7vhXfQJf1i0C/O9/PWyGO5/vThnPaKYKb4eel0uPeJtXzn3is4+f+9ixUK8Nz3T+Hls77Bf9JP4ONnn+GoUy7irlOLePHmR4k4inO/exqvyRgWPLsYgMlnzOKJ59fSWlvOkGPn8PPzxmM//wc+/O86docspuWkMvH6E2idfD4Pvr+N444dzHmjC7DfeZpN81ewojFM0FYMTPUwalwBpadPh7GzWbY7wKtrdjMyw8egY0sonDUVZ/hUNteH+XBbPZu3NtBYWUOovhIr1AJAZNNK6tdto25TLQ0VLewOWe00er9pkG4a5PlMmndU0VLRTKAqQGPIojHqELCdvYzWTHGTs1rCVDWHqY0brQUtImGLaKgVOxLEDgdxrAjKsTEycjDSMiElHcebhvKl4XhTCceTshxFxHYIW85eiVmG19em8ceTs0yPB9M0EEPajNaUo3BsV8t3E7TiiVnKsVG27Wr2RltiVmcaf/xYnO6M1pRtuz+b7o3WEvX8ZBOzEtnXL1Zvea91NuYciLHb4alg7x8xl81eGfQHATsStne6+/Z+TZE04GzgmYTdCnhVRJaKyE37927ao+UdjUaj2Yt9B/oTKOigtT+glHqg3Y32RnWyD+ACYHEHaWe2UqpcRIqABSLyiVJqUTId6wo96Gs0Gk1HXHknCWqUUtP3cXwnUJqwPRgo7+Lcy+kg7Silyt1llYg8S0wuOqBBv1/IOxqNRnOw6SV5ZwkwSkSGi4iP2MA+b6/XEskGTgaeT9iXLiKZ8XXgTGD1gb6vfjHoB4uH8n5dkIzHnufdxx5l3p9u5KHjbuAzY/N5afpXWPbs41xx89Wk3/tt5u9s4vu/f4WUzFwe+PpsNt1yI69VBbj4mBKyb/09tz+2lLqyFQyZcQb3fGYSFcteI2fY0XzhovFMjWxg6d0vsqQ+xIBUD9PPHkHOZ77Ifz+p4e33tnPDzKEUVy1n67OvsWZ9HbtDFtleg4l5foaeNpa0WeeyLZrO6xuq2byplqFj8imZNR7fpJPYTRYf7GzkvY011O6OzdGPBBoB8KRm0LJhPXUbymnc1sSuoEWT5bQrhp7hien5BSkeWnbW0FzRQkt9iLqITcB29jJaM0XwmwaphtFmtNYaiBAJRl2ztQhWsCU2R9+KzdG3oxEMt4CK8vldszV/m8FayF22Rm1aozZmQuGUNmM1j69t2/D4MFw93zSNmMlaQjF0225vvBafb68cu20OfrwYeuK8/MRtQyRpo7WOdGe01hN5PP56Ha/pqzn6h+kU8kMGETA90m3rDqWUBdwMvAKsA55USq0RkS+LyJcTTr0YeFUpFUjYVwy8IyIrgA+B/ymlXj7Q96blHY1Go+mE3qp2ppR6EXixw777O2w/AjzSYV8ZMLlXOpGAHvQ1Go2mAyJy2Gbk6kFfo9FoOiHZjNv+hh70NRqNphMO10G/XwRyt27bzU/e/C0n3fAnZl19Dfm/upGtrVFOfv8Vbv7RPxgy63zun27x1zsXMndoNlVrF3PRFy5h+tonePzJtUzOTuX4+3/MrfM/Yf3CWDWtr14+idHbF2J4fEw6bQZfmzGYsj/8P95cWQXASaPyGHXjlayVgTz8+mZ2r1nKcXk21c89wcaXy9jQEsYUGJ3hY/icoRSdfhpNReN5a2sdb6+upHrLTgbNHkHWcScSLBrDysoA72yspmpnE00VWwk11uBYEQyPj5TM3Fhi1sY6djeEqHEN1GwVS7Dym0KWx6AwxSS9OI3mihYClQHqIjaN0c6CuLFrUt0AcHVziMaWCKHWKGHXaC0exLXDQexICDuenJWehfL63ZZGVDyELYeQWzUrFHVojTpthmuJrTOjNcMN4poeI5acZTnYlmoL6nY0WosnULVVzOrCaK2tmlbC72XHIG4i8fsCbYHbzognZsXl3GQSszoGcfdltNZbiVmdoROzehGJfU66a/0R/aSv0Wg0HRAEw9Mvnol7jB70NRqNpiNy+For60Ffo9FoOqG3pmweavSLv1+8aZmc8W4+htfHwnPgDw9+zO33X8Xsuz8m3FjDSz85gxdPuB5ThDNfvIdRcy7mgbMH8L8b7qPFcrjkB2ewwD+Vef95C+XYHHPOiXxlQgYrfvonhs86g/938dE4//0t7zy+inLXaG3yTScTnH4x9ywqo+zjTwhU78Be9ATrn1nKxw0hgrai1O9l3MQihp5zHEw8lY8qArywsoKKLXW0VG6lePYxqJEz2FQfZnFZLRvK6qnftbud0ZovPRt/7gBq1ldTW9650VqWxyTPZ5Kdl0pmSQYtFS3UBaPURRw3Mau90Vq8eIrfNMjwCFVNYUKtscIp4WC0ndGaHQmhHBsnGtP08WfhpGS0M1oLJRitxROzwpbTzmitTc/vYLRmeGJNDLo1Wov3oS05yzS6NFrzmUZMVzWkS6O1eGJWop4fu3dyRmv786CXrNHagfziHW5Ga4fi2BozXOu+9Uf6vNsiYorIMhF5wd3OE5EFIrLRXeb2dR80Go2mR7jyTnetP3IwvqtuIZZ+HOd24HWl1CjgdXdbo9FoDiEEwzS6bf2RPu21iAwGzgMeStg9F3jUXX8UuKgv+6DRaDQ9RfST/n5zN3AbkCi6FiulKgDcZVFnF4rITSLykYh8VJwS4r1/PsY7D36Ju2bcxFUzB/HY2OtZ8dwT3PL9G5Cf3cALFc186cdn8ZuKgTzx3ZNY84XreK0qwOVzhuH5yp1896El1JWtYMTss7n30klU33MHL76xjZsvm8jE+qV8eOcLLKkPUur3MvPiMWRd+lUeX13FO4u3Ub91NabPz6bHX2bZulp2hyzyfCZTBmQw/OyJpB5/AZtCqby4tpLNG2pp2P4JocZqfFPnsMtO570dDby/sYaa8ia3GHrMLtuTmkFqbjGZRSU0lDWwK2i5xdDbG60VppgUpnlJL0onc3A2jXWhBD1/72Lo8YIrGR6DbK9JKBAlFIgQDkaJBINYwRaioRbXaC2CnaClJxqtxebnx0zWwpaiOWy3afqtUTtpozXTE1u2zdPfh9FavPnM9jp+Z0ZrplvgHHrfaM2Q5LTmrubx78to7VDS8z9tDuWu91aN3EONPhv0ReR8oEoptXR/rldKPaCUmq6Uml6Qn9/LvdNoNJquEaHzhMAOrT/Sl1M2ZwMXisi5QCqQJSL/BCpFpEQpVSEiJUBVH/ZBo9Fo9ov+Oqh3R5896Sulvq+UGqyUGkascMBCpdTVxAoIXOuedi0JRQM0Go3mUEDo/im/v34pfBrJWb8BnhSRG4DtwKWfQh80Go2mS0TAp20Y9h+l1JvAm+56LXBaT66vWb2eG554jNpLzwdg1Muvcu4F/8fE8y/jDv/H3H7/Eq6aOYi6637NXTf8ia9e0MzPXtjInMI0pj90Nxf8azmb3nqB/JHT+L/rjmHwkn/y1J8WUR6yuH1sGmu/8nteW1+LzxBOmTaAkV/7MotbMnn4lY+pWPUediRIwehjWfv6G2wORPAZwtFZKRx15lEUnnku1TkjWbC6kndX7aZmyxZaa8tRjk1jzlEs2dLAa2sr2b2tgaaKMkKNNbEEIZ+f1OwC0guHkFucQXlTmJqI1c5oLcNjkOs1KUwxyRyYQdbgTDIGFVIXWUtj1G6XxAV7krLiRmvZXgO/zyTUGmlLzGqrlhWNxIzWorFg7p5AbjrKm0ZEPK7JmkPYUu0CuK1Rm4BruGZ4fG4FrT1BXNMTM1iLG62JxHxMImHbNVdrb7TmWBGU3T6Q25XRms9jtBmted0ErX0FcTsmZnVFotFasg9wHe+XjNHaoTaM9ORZtbcNxg7pIK6Ap58+yXeHtmHQaDSaDgiHr6avB32NRqPpiPRfzb47DrW/NjUajeZTJ/akb3TbkrqXyNkisl5ENonIXg4EInKKiDSKyHK3/TjZa/eHfvGkbyv4TeNT/ODt7fxx9SMc9e0XSC8s5d3vzeL+gTMYl5nCcS89y8Q7FtJaW87fb1tArtfkwgdu5J7tGbz79FP40rO57MqT+UxWFW/d/ncW1waZnJ1K7V9+yoIXNlEXsTm/JJOp35zLjtLZ3Pn0KrZ89DGhxmoyS45ixLSxfPxsiIijODorhTEzB1F6wWlY409l0cZ65i3dRUVZFS2VW7EjQTypGayqauWNDdWUba6jsXwXrbXl2JEgYpj40rNJLxxCTmE6gwZmsju0p3AKtNfzs4vSyRqcSdaQIjKHFFMXsQm4SVkdjdb8blJWhscgy2viz00lHLQIh2J6vh0Juss9hVPiDcBJycD2pBKKxrT8sK0IWU6Cnu8QthyCETum4SeYrBke356iKXGzNVPa9H3HTcyyLQfHdrAtq61wSsfkrLjRWsekLFMEbzwjMqGISneFUxKPd2W0Jh00eGM/rMg6S5TqLe3600zM6q8FQw6E3njSFxETuBc4A9gJLBGReUqptR1OfVspdf5+Xtsj+sWgr9FoNAcTQ6S3Zu/MADYppcoAROQJYlY0yQzcB3Jtl2h5R6PRaDrBFOm2AQVxuxi33dThNoOAHQnbO919HZklIitE5CURmdDDa3uEftLXaDSaDsRtGJKgRik1fV+36mSf6rD9MTBUKdXiOhg8B4xK8toe0y+e9AdMGMEPbvwnP/rleZz2ilC5ahHz/3ANi48/g/JQlOte+BlnPfIJW96Zx3GXX8bW1ijXfGM2K6Zey+/ve51gfSVTLziHO88awerbvs+L62oYkOrhjKsnsegPb7ChJcK0nFSO+fpJcO7N3P32Vla+vY6mnRtIzS5k8KSpfP6UETRGHUr9XiaNzWfUJbMwZlzAkopWnlu+i+3ra2jcvpZwcx2Gx0dawUDeKqtl+YYaanfV0FK5lWigEYgVTknLH0h2cQFFg7KYNjTXNVqLF04RsjwxPT8/N5WMgRlkDs4lc0gxvkFDabJihVPic/T36PlCuhmbn5/tNUjJ9pGam0ooECHSGsAKtRAN7jFaSyxaEkd5/YSsmG4fsmMF0VsiFi0Rm6Cr67eELVpC1p75+e4cfdPjiVnOuoVTTI+0m6/vKHdufkLhlM6a487T38twLaFwSnyufkenw84Kp3Sku8IpB2K0lngf6LpwSk+1eG20dvDppYzcnUBpwvZgoDzxBKVUk1KqxV1/EfCKSEEy1+4P+klfo9FoOtCLyVlLgFEiMhzYRcyS5sr2ryUDgEqllBKRGcSeD2qBhu6u3R/0oK/RaDQdEHonkKuUskTkZuAVwAQeVkqtEZEvu8fvBz4LfEVELCAIXK6UUkCn1x5on/Sgr9FoNB3ogabfLa5k82KHffcnrP8Z+HOy1x4oetDXaDSaDhzONgz9IpC7tjrKlceX8uRJ3+bdxx7ltp99nZxf38iTq6r41i/O4zetk3nvX/9mxElzefnLM7jq1GGk//Av3HDPYqo/eZ9Rc+by92uPoebOW3l+/kZspTj35CEM/94dLKpppdTv5eRLx1Nww3d5eHkFL762iZoNSzB9forGH8cFJw/n4rEF5PlMjinJYNRFU0k/9TNssrJ4ekU5q1ZXUbdlLcH6SsQw8ecWk1M6moWrd1O1vYHm8k3tqmX58zPki0oAACAASURBVAeSNWAw+SUZTBuay8SSrHbVsrLdpKzCNC9ZgzPJHpJD1rASUktL8Q4cllS1rLSsFFJzUvHnprarlmVHgm1Gax2DuAAhWxG0FKEuqmUFIrEgbmvE7rRa1p7A7d5JWonJWW1B22hkryCucuxOE7MSq2XFE7S8hnRqtJZIx/eYTLWsjsla+7rfnnu0N1rrrSDuvl7rYHAkGa21oYuoaDQazZFD3E//cEQP+hqNRtMJetDXaDSaIwTjMC6i0i/eVaipgZyn/sft3/o9s66+htvqnubuv37Ely8Zw6qLf8zvf/0oeSMm8/wP57DxC59h2r8f5ZK/fsCmt+YxYPIc7v7ScRQvuIf597xNecji3FF5TP35rbzUUkSGx+CsU4Yw6rbbeKkmlYfmr6N8xSKUY1Mw+lhOOGEY1x4zmLytizk2N5XRF46jaO6l7Mo8innrKlm8vJzKjesJVO+IGYVl5pE1eAwDhuaye2sDDds/IVhf2VY4xZ9bTGbxUAoGZTFxWB6TB2czpiANW8X1fIMCn8mAVE9Mzx+cRdbwEtKHDMJbMgyVO7DTwil79HyDdL8Hf25Mz0/NTd2j54eDOFZ0r8Ip7X7WtiLcoXBKc2RP4ZSWkEUwEkvQ6qxwisdrtun6bUlartZv284+C6c4TvsiKomJWV7DaFc4JW64Ftebky2ckrjdVeGU/dHz267t5Lre1vN7+voHdr8jUM8HrelrNBrNkYTQ5q1z2KEHfY1Go+mEw9VOWg/6Go1G0wGBtloNhxv9QtMfXDqAE6+/m6Ezz2ThOfCzax/mwpF55D34DFff/m/EMLj3jotIv/fbPPzUOr7wShVL//ssWYNH88OvnMRJVW/w6jcfZ0VjiNOL0jnhzmtZM/AkfvrkCs6eUMikH36Jpb4x3DlvLVs/fJdooJHcYUczftYovn7iCEYENlL+xOOMPecoBn/2IhpKZ/DSxlrmf7CDig3baNm9FceK4E3PJmvQaAYMK+T48UXUbd9MsL4Sx4pgeHykZheQMWA4eSWZjBqSw7ShOUwoymBQhrddIfQBqR6yBmWSMyybrOEDyBpWgmfgcKRgMHZmcVvhlESTtbien53qIdXV8tMK0kjNz27T87sqnBJHDJNg1CHk6vktEZuWiLXHaC0Um6PfHLYIRqx2er7Ha7Zp94Ype7T8hDn7ylHYltWm5zv76Eu7efoJ5mqGCF4zYc6+IT3W8zsWTkmcV9/RfK2z67uis0Lo7X6+vfTk2NV9DnU9v18R/7x10/oj+klfo9FoOiCAN8lyiP0NPehrNBpNBw5neUcP+hqNRtMR6b/yTXfoQV+j0Wg6IBy+MY1+IVrlNFaQml3Iyp8cx10zbmJcZgqnfPwGc37wCk3lm/nhHddxxooH+eudCxmY6mXew//F68/g+i+ey40Flbz1xTt5pTLAtJxUTv/5XKpmf4FbnljOhrfeZOZPr2L76HP4/rw1fLLoPVpry8ksOYpRMyfxrdNGMdlTTc1Tf2ftk8sZ/rnzsaZdyIKyep54bxs7PtlFw451WKEWPKkZZJUcRfGIQUwbX8ScUQUEqndghVoQw4wFcYuHUzAojxFDczhuRB6Ti7MozfSS2rC9LYg7yO8hrzidnKFZZA8rJvuoQXgHj8QcMBw7u4SApAKJ1bL2BHFzfbGkrLSCNNLy/aTmZ5Kan4UVbGkL4joJiVmdEbYVgYhNYzgWsG2J2DS7Jmtxo7VgxG4zXPP4vHsZqyVWyzI9gmEaeDwGtmXFgrZ259Wy2rZte4/RWjtztViCVjyIG0/UirM/SVkd97WtH8Dve1dGa4ns7/37cxC3v42hMXO/fbf+iH7S12g0mg6I+1BxOKIHfY1Go+nA4Szv6EFfo9FoOqG/yjfd0S/+fqnY3cyqh67lP6PmAHD1iqeZ8YvF7FzyMld/8wt8w36XP9/0D0wRbrzrs1iRIOdddzG/PDaV9679Ns+tr2V0ho8LbjsN+4ofcfMzq1i1YBHB+t00nHQDP/zfOla/sZTmis2kF5YycuYMbj17DHMKLZqf+xtr/vkBH5Y3I7Mv47UtDTz23ja2rK6gYetqooFGTJ+fjAHDKDpqBBPHFXHm2CKmlmQQDTS6en4hGcXDyS8tonRoDsePKmBqSRbDcnykBypRO9a5SVkm+YXp5I7IIXt4EdkjB+EbPALPwBHY2QNo9WRQG7QxhTYtP8tjkOczyfOZpOam4i/wk1bgx1+QSWp+NmlFudiREFYkuE89XwwTMUwCEYfmyB49v11SVsiiJWzRHIoSjNiYHk87YzWPt73pmsdrtBVW8XmMdkVTEhOzOur5ccM1X4K5WlzP95h7dP24tg/J6/mwdwJWV3p+4u98d4lZbT/HJAqnHOp6fl/Q3x6ahT2GfvtqSd1L5GwRWS8im0Tk9k6OXyUiK932rohMTji2VURWichyEfmoN96bftLXaDSajvRSjVwRMYF7gTOAncASEZmnlFqbcNoW4GSlVL2InAM8AByXcHyOUqrmgDvjogd9jUaj6UBM0++VW80ANimlygBE5AlgLtA26Cul3k04/31gcK+8chf0C3lHo9FoDiZxG4buGlAgIh8ltJs63GoQsCNhe6e7rytuAF5K2FbAqyKytJN77xf94km/KDeV98fPZHMgyh1LH+KEx3az7pWnOesrN3LfmCoePPlX1Edtbv3pOWw8+7ucYK3lkQuHsuLKK/jPezsZmOrlkq/OIvvW3/OlZ1bz3vy3aKncSsHoY/nRyxt4+6Wl1JWtwJ87gBHHzeKr543l/KGphJ65m1WPLOK9TfWUhyze3h3l0fe3sWHlburLVhBqrMbw+Fw9fzRjxxZw9oRijh2YSUFrOQApmXmkF5aSO2gAA4fkMHtUAdNKshmRk0JWqAbZuZbQppUMSPUwoDAtNj9/eAG5o0tJHXoU3iGjsbIHEkzJpabVYndLpJ3RWrY31tLyYlp+WkEa/vwM/IW5pBXl4s3JwbF2tytA3pG4nm94fLREYlp+MBozW2sJt9fzg5FYEZVgyMLjNV0t34yZqiXMzzdMadPz/T4Tn8fYqwh6V3q+cuw2Pd9rdq7nexPW96Xnd0XHQuiJ++DI1vOP2MIpiQgkOWOzRik1fd932gvVyT5EZA6xQf+EhN2zlVLlIlIELBCRT5RSi5LqWRf02ZO+iKSKyIciskJE1ojIT939eSKyQEQ2usvcvuqDRqPR7A/xKZu9EMjdCZQmbA8Gyvd6PZFJwEPAXKVUbXy/UqrcXVYBzxKTiw6IvpR3wsCpSqnJwBTgbBGZCdwOvK6UGgW87m5rNBrNIYS4lt77bkmwBBglIsNFxAdcDsxr90oiQ4D/Ap9XSm1I2J8uIpnxdeBMYPWBvrM+k3eUUgpocTe9blPEghinuPsfBd4EvtdX/dBoNJqe0lvJWUopS0RuBl4BTOBhpdQaEfmye/x+4MdAPnCfK+NZrmRUDDzr7vMA/1ZKvXygfepTTd+drrQUGAncq5T6QESKlVIVAEqpCler6uzam4CbAErSUiG9L3uq0Wg0e4jZMPROMEIp9SLwYod99yesfxH4YifXlQGTO+4/UPp09o5SylZKTSGmY80QkaN7cO0DSqnpSqnp6cNHs6iyhR8svJPTXhGWPvUvZl97Hc+fZvLPU29hQ0uYr337ZOqu+zVX/OYN5l07iXU3Xcu/Xi0jz2fyuS9MpeSOP/Lt/63nlWcW0bRzA3kjJnPKedN5Zf7H1GxYQmp2IcNnnsBN54/j8rE5WP+7j1V/e4N3V1ezIxglw2Pw8HtbWbm0nJoNHxOs350QxB3LmPGFzJ08kONLsymOVGKtWkRKZh4ZxcPIKy1l4LBYEPeYQdmMzEslJ1qP7FxLeMMy6lZvoSTfT+7wHHJHFZI7egipw2JBXDt7IOG0fGqDFlWBCDsag25SlkmeL5aYlZGbSlqBn/TidNKLMvEX5uLPz8aXn4eZW4QVDrYlRHUkMYgrpklj2E3Aiti0hC0aW6PtgrjNIYtwxMaK2u2CuPHKWR6fiWFKW4JWvPpVipuclZiY1VUQF2gL4iZWzeosiNvdXOpOA9cdgrh7ma+5S0Mk6SBuIr0dxO3ydXQQt08R6b71Rw7KlE2lVAMxGedsoFJESgDcZdXB6INGo9H0BAPptvVH+nL2TqGI5LjrfuB04BNiQYxr3dOuBZ7vqz5oNBrN/iAcvk/6fanplwCPurq+ATyplHpBRN4DnhSRG4DtwKV92AeNRqPZL/qDp9H+0Jezd1YCUzvZXwuc1pN7lW3dzc9evYtzlhTx7mN/Z9bV1/DaRVn8a/oVrGgM8Y1bT6D11nu45BcL2f7eC2z44kP849n1ZHtNrrpuCqW/foBvv7KNZx9/k4atq8kZdjQnXzCLX583jlF/+AspmXkMn3kyX7pwPNdOLMCe/0eW3/sK7y6vZGtrFL8pTM5OYf6SXVSvX0prbXmbnl84cjyjxhdy0ZRBzB6SQ0m0Gmf1ImoWv09G8WTySksZMCyHk8YUMmtoLuMK0si36jF2rSWyYRm1KzdTu24XuSNien7e2GH4jxqFb9hY7NxSwumFbUlZ2xtDbG8IkuUxyfbu0fPTi9Jjmr6r56cV5ZJSVICZW4SZW5S0nm96fG16flMo2k7PbwlF2/T8SNjCijpJ6fl+n0mKx8DnMZPW85Vjt+n5ewqoSKd6vjfhN7M7o7X4vq70fEPa6/n7Q7J6/oGOJ1rP72P68ZN8dyQl74jIJW4yVaOINIlIs4g09XXnNBqN5tNAem+e/iFHsk/6vwUuUEqt68vOaDQazaHCkS7vVOoBX6PRHEkcpmN+0oP+RyLyH+A5YvYKACil/tsnvdJoNJpPkcO5XGKyUzazgFZi3g8XuO38vupURzz+DM5eXsrbf48Fcd+4OIN/HHMFHzeEuOXbJxH8zr1c8LPX2fLOPIbMOp9Hn/6EDI/BVddNYchvH+LWV7bx9L/foK5sBXkjJjNn7gn87sLxFC/9DymZeYw4/lS+evEErp9UiDP/jyz704u8vWw3mwMR/KYwLSeVSacOo3LdR3sFccdPLOYz0wZz0tAcBlrVOKvepPrtd9n17ibyhw5jwLAc5owr2iuIG177ITXLN1C7bhe1G+vJH1O0VxA3lF5IVatFRUuErfVBtta1UlYdIM9nUJiyJ4ibXpxORkl2p0FcI7sg6SCu4fEmHcR1LKdHQVyfx0g6iKscJ+kgbvwXM9kgLiQfxO3p77wO4h5eHNFTNpVS1/d1RzQajeZQ4nAtNpLs7J3BIvKsiFSJSKWIPCMifVrdRaPRaD4txC2X2F3rjyT7ZfZ3Ypm0A4lVfZnv7tNoNJrDksNV3kl20C9USv1dKWW57RGgsA/71Y6jB2ey+NFHmHPjDSw8Bx465mpWN4X5zh1n0vCNezjvx6+y7d35jDhpLo/fPocsj8E1X55B6V3/4Evzy3j6H69SV7aC/JHTOPszJ3LX3AkUvvsoS/7vYUaeeBq3fPZorhufTfTp3/HRXfN58+PdbG2Nmawdm+tnyhnDGX3lmW16fubAoygeNYFJkwdw6TGDmTMsh0GRCuxlC6h68x12vL2B8lVVDBqRy+kTijlhWB7jC9MoiNZi7FhNaPX7VC/fSPXqndSsr6WqKkD+hBGkjRqDb8QErLyhhNILqW612NUU0/O3uHr+tppAu6SsRJO19JL8PXp+/gAkpwjHn73Xz7MrPd/w+JLS861ozHCtJ3q+zzSS1vOBpPV80+iZnh8nrucb0jt6fruf76eo5+/P/bWevzdCbHDsrvVHku13jYhcLSKm264Garu9SqPRaPopItJt648kO+h/AbgM2A1UAJ9192k0Gs3hR8Jfgftq/ZFkZ+9sBy7s475oNBrNIYEAvVRD5ZBjn4O+iNymlPqtiPyJTiq4K6W+0Wc9S6Bu1XqufOxhHijdwF0zfkyTZfP9P3yGZWfdxvW3P0fl6kWMO+uz/OebJzDg+d8w8PbT8H/rD1z+rxUsevpVWiq3UjR+Nhddciw/O3MkqS//mfd/9QwL19Xw/b9O5qJSg8A/f8OyvyzknY11lIcssr0xPX/CuUcx/HPnY8y6BPPX/+fq+WOYMqmYi9yiKYWB7USXLaTy7Q8pf38L5Z/UsqklwlkTBzCzNIdReX5ygpWwfRXBdR9Ts3IztWvLqd1UT1VdkN0hG//IsXiHjsXKHUyrL4eagMWupjDbG0NsqQ2wrbaVbTUBWhpCZOankV6cRkZxOmlFWW16vi8/HyOnCDO3EMkqwPFnozpo+ol6vun1ueteTJ8fw+ujsTVKQ2s0ZrwWihKM2ARDlqvju3p+xMaxFf4MH6bHwOM1MEwD09Xy40VTfB4ztm3GtpPV85Vj40nQ8L3t1mOeJ3E9P1GP7qrgyb70fGiv5++Zw79/aD3/8KG/yjfd0d1nO2698BGxsocdm0aj0Rx2xDJye0feEZGzRWS9iGwSkds7OS4i8kf3+EoRmZbstfvDPp/0lVLz3dVWpdRTHTqqffA1Gs1hS28857v1RO4FzgB2AktEZJ5Sam3CaecAo9x2HPAX4Lgkr+0xyf4V+/0k92k0Gs1hQExC7K4lwQxgk1KqTCkVAZ4A5nY4Zy7wmIrxPpDjlpJN5toe052mfw5wLjBIRP6YcCgLsA70xTUajeaQJPnkqwIR+Shh+wGl1AMJ24OAHQnbO4k9zdPNOYOSvLbHdDd7p5yYnn8h7TX8ZuCbB/riyRJxFH9WL/DzMx4ly2Pygydv4fGBF3H7bf+gaecGjrn0Kp792kzsu7/Fg797g8u2fcwlDy1h2fxXCDVWM+jYc7n+0oncdsIQQv/4OYvufInXtjfSYjlcUhig9sE/suyv77B4VxPVYZvCFJPj8tIYe8k4Sj87FzXjIt4pbyVnyDgGjh3JjMklXHj0AGYMyiSndgPhpa9Rsegjdr2/nZ1lDWxqiVATsbl2WD5H5frIaNqBs2UFrWuWU7umjJq1ldSXNbC7IcTukEV91MYz/Gis3MG0mBluUlaYrQ1BttW2UlbdQnldkJaGEIGmMJkDM0gvSiOtKBt/US7pA/Lx5sdN1gohIx/Hn43jz8b2prX9HNsSsgwT0+vDSEjKMrw+PD5/uyBuS8giErE7DeJaEbtdENfnBnB9HoM0n9kuKSvF3d9ZEHdPIHdPEFc5NqaA1zRiAdt9BHHjhSySDeICSQdxexrI60kQt6fT/foiiKvpGlEK6eIz1YEapdT0fd2qk30dJ8V0dU4y1/aY7jT9FcAKEfmXUko/2Ws0miMGUU5v3GYnUJqwPZjYw3Qy5/iSuLbH7FPTF5En3dVlblQ53laJyMoDfXGNRqM5NFGgnO5b9ywBRonIcBHxAZcT8zFLZB5wjTuLZybQqJSqSPLaHtOdvHOLuzxo3vkajUZzSKAOWElBKWWJyM3AK4AJPKyUWiMiX3aP3w+8SCx2uolY3ZLr93XtgfapO3mnwl2tAYJKKUdERgNjgZcO9MWTpWTCcH5wzd+Zmefnc2/dx3c3FvC32+7HsSKc/aXr+M/l4yj7+hX8+/E1MZ3+7ndY99qLKMdm5MkXcttVU7hyiKLqt7fy3n3vsKimFYDZ+X52/P5nLP/nxyyuDdJiOZT6vcwYmsXYz0yh5DOXEhh9MgvLGnjiox0MnTyWOVMHcu64Yo4pSSd1x1IC7y9g16LllH9YzpadTewIWtRFbCKOYlxBKik1G7E2LqN59QpqV2+hdn0N9WUN7GqJUB22qY/aBG0Hq2AEjY6X6haL7Y1BtjeG2FoToKy6hcr6IIGmMK2NYVpbwmSWZOAvyiF9QB7+oly8BcUYbtEU0nNwUrNxUrOImim0Ruy2hKx466jnmyl+13TNR2MwQnPIIhixCYctrMgegzXbctoKqNi2g8dr4PGaeNpp+Uanen6bpt+Fnp+YpAXt9fzYOp3q+YZIj/R8aK/ndzRY2189v7P7x19jX8d7A63n9wFKJfskn8St1IvEBvbEffcnrCvga8lee6AkO2VzEZAqIoOA14l9Ez3Smx3RaDSaQwlRTretP5LsoC9KqVbgEuBPSqmLgfF91y2NRqP5NFHgWN23fkjSg76IzAKuAv7n7ku2qLpGo9H0LxS9Fcg95Eh24L6VWAbus24QYgTwRt91qz3ram3+39QBHP/6fE57eA3v//vPZAwYxjdvvYTbR7Tw7hnn8tSH5eT5TL5w6Tjue/EZUrMLGXfqqdx5xRROpIwN3/8Vbzz9CSsaQ2R7DU4sSGfyF2bw6n2LWdEYxlaKcZkpTJ9UxJjLZpB7wVVUZI/m5TVVPPHhDraureJLn5vE2aMLGZ2hMNe+Tt3ihex6Zy0VS3ezqaaV8pBFY9TGVuAzhNTylYTXLaFh1VpqV2+lblM9tdsa2RW0qInYNEZj2r+toMbyUt0aZUt9kO2NQcqqAmyrDVBbH6S1KUygKUwoECLSXEfGiALSSvLwF+a2FUwxc2MFU5yUTJQ/mxAeWiMOgaizx2TN62tXMMVwtX2Pz9+m7Te0xkzWovGCKREb23ZcTV9hRe0ETd8kpZ3BmoHf58FnGu32+TwGpiE40ViB9u70fMexY/Py3ZJ0iXp+x7n6XdGVng9dF0zpqOcf6Fz6A52bnwxaz+8rFDj9c1DvjmStld8C3hKRTBHJUEqVAQfFYVOj0Wg+DfqrZt8dyRZGnygiy4DVwFoRWSoiE/q2axqNRvMpcoTLO38FvqWUegNARE4BHgSO76N+aTQazaeHUpCcDUO/I9lBPz0+4AMopd4UkfQ+6pNGo9F86hyu8k6yg36ZiNwB/MPdvhrY0jdd2ptgYz0lyz9i4o8WsuWdeQyZdT4PfOtEZn3yJM/OvI/XqgJMzk5l7nfmkPudP5B9xX2ceMFs7po7gQHLnuKDXz3Cgvd2UR6yGJjq4ZRJRUy+6VTSLryJJb88Cb8pHJvrZ+LJQxh9+Wl4T76MT+w8nlm6i5eW7GTXhnIat6/jsqPPZKBVjfPBm1Qufp9d721k94oq1jdHqAxbtFixD4nfFAp8HkIfvU7N8g3UrttF7cZ6qqoCbQZrjVGHiBPL+DMFtjWGYglZda2UVQfYVhOgqSFEa1OY1uYw4UAL0UAj0VALmUOKSSmKG6wVYWQXtBmsOSmZtFqK1qhNwHIIRp2YyZpp7hXEbQvgulWzPL4UWkMWETeI61hOm9ma7QZv40Fc23JI8cUqY6V0SMjqGMRNTM6CvatkJS6deHKWG8T1Gp0nZMW3O+ZQ7SuAm0hvB3E70tdBXB3A7Wt6LznrUKMnhdELgf+6rQA3VVij0WgOS45ETV9EUoEvAyOBVcC3lVLRg9ExjUaj+dToRRuGQ43u5J1HgSjwNrGSXuOIzdnXaDSawxbhyNX0xyulJgKIyN+AD/u+S3szcPAAjr/uT7TWlnP8Ndfy3I3HUv+TL3HXfe9THopy8ag8Tv7z1yibdClX/uUDbv/WXL42JZ+mh+7gtbte543dLQRth2k5qcw6ewSjb7ycyMxL+fe6GgakejiuOJ0xF09g8GWfwZ56Hgu3NfHkss18tLyCyo0baSrfjBVqobTxE0JLFlD+9jJ2vr+DHVsb2BKIUuMarJkCGR6D4hQPg/weyt9eRvXaShrKGihvCrM7ZNNk2bRYDrZr4GcK+E2DtVUtbK1tZVttgJ01rbQ0hgg2Rwi2hAk3NxBpbcQKtmBHQqSWlmLmFroGa7nYftdgzeOnNeIQtBSBqEMgYtMYtrosmBJPyIolaHkxTYNw0IolYtmxxKxEgzXbcnBsB9uyUI6N32d2WTDF1yExy2cadFUwJU5cz1e23WXBlI56vpGgbvdEz9+XwVqbIdt+CufJ6PkHYuim9fyDgQL78Jy9052m3ybl9LSIioiUisgbIrJORNaIyC3u/jwRWSAiG91l7n70W6PRaPqOw9iGobtBf7KINLmtGZgUXxeRpm6utYjFAMYBM4Gvich44HbgdaXUKGKOnbcf6JvQaDSa3uZwddnszk/f3N8bu178Fe56s4isI1body5winvao8CbwPf293U0Go2m9zlyA7m9gogMA6YCHwDF8eIsSqkKESnq4pqbgJsABmVn4D06g9/d/R2+kr2Nt44/hWdWV1Gc4uHrX5jCUb/4PQ9v83DXLxey/cMFvH7Wpaz78jd4ff4m1jWHyfOZnD4kl0nXz6T46i+x0T+CBxZsZsHibTwwaxDjLp9N5lmfY2fGUby4fDdPvr+d7Z9UU1e2ktbacpRj40nNoPb5f7UZrG2uD7EjGG3T532GkOczKU7xMCTTR+6IHHa+v4OaHU3sDu0xWAvae6rx+Awhw2OQ5TFYvqORbbUB6utDBJpCBFsibQZrkdZG7HAQKxTAjkbwFJfuMVjzZ6NSMgkqk6BrsBa0HBqCFi0RK6bp+1K7NFgzPD48XtMtcm66RmtxTX/vufnKsXGsCE40Qmaqd59z801D8HkMvIaBKe21/MSlk6DFK1dHNV1ztc60/NjnI6bniySv5cfPS2Zu/v5I7n2t5Xf2GgeTA+x6/+MwHfSTnae/34hIBvAMcKtSqjtJqA2l1ANKqelKqen56f6+66BGo9F0JG7D0F3rh/TpoC8iXmID/r+UUv91d1eKSIl7vASo6ss+aDQaTc9RKCvabTtQkpnY0tWkGPfYT0Rkl4gsd9u53b1mnw36Evs79m/AOqXUXQmH5gHXuuvXAs/3VR80Go1mv1AcrCf9ZCa2dDUpJs4flFJT3NZtPd2+fNKfDXweOLXDt9BvgDNEZCNwhrut0Wg0hwwKhbLtblsvMJfYhBbc5UV79UWpCqXUx+56MxCfFLNf9FkgVyn1Dl3HnU7ryb3KyxtZ9fAXse/+Fnf97g02ByJcMDiLU/90Pbtmf5Fz/rOC5a+8Q9PODWSWHMWCC77Ja9sbabEcjs5KYfacoYz78mdRp1zDU+tr+etzy9m8fBu1mz5m2i9uQs24iLfKW3nqjc28Tw9/AgAAHxJJREFUv6yc3Rs201SxmWigETFM0vIHklkyknVPPMDOsgY2tUTaJWRlew0KfLGErAElGfz/9s48Sq6zvNPPe29VdVd1t3pvtRZLLcuSJWGD8SJwDMYEG4wHW8BgYw8BzhyCyUyYMwQIcfAMSyBzHJIY5kwIxHZwyISwx6w+Nl6wPXYAY8mSLFkS2iVr7W6pS93Vtd17v/nj3qquqq7q6pZ6K9X7nHNP3fvVXb7Pbr19+/duHava6Vi9mCfv+3W+wFq5hKycE7cjYvPYkTgjQym/Q9ZohvTwGbKjcTKJOG4mhZNJ4mUzeE4Gq3uZn5AVbcUNx0hkPUYDB+5w2mU44xBPOYxkXOLpbEFBtWiQpOU7cXMJWaGwjRWyCIUtMmkHzzX5jlmlCVleNpN35kYjdtWErLAlWJYQtvz3i4kSsvI/O55b0Ylb6MCFyRcyK3zmZBOyzuWN6HxLyKo/Jy6T7ZzVJSIvFBzfZ4y5bwpPmlRgS46SoJgcHxWRDwAv4P9FcHqie2ifW0VRlHFMup7+gDHmyolOEJHHgd4yX909lRlVCIr5GvAF/F9TXwD+Fr9AZkXU6CuKopRizLQ4av1bmesrfSciJ0RkUfCWXzGwpUJQDMaYEwXn3A/8rNp8ZjxkU1EUpfYweSlyom0aqBrYMkFQTC4CMse78FvaTkhNvOl3tzWy+fI38KN9p1nZFOFTH/s9ln7mXu7dPMz9//MXHNn4GFYowoXXbuD9t6zlR9ffT3eDzdsu6uSyO6+l/bY7eVkW87Wf7eKZfz/EsZdfJNF/GIBD627hpxuP8+PnD3NwxwmGDrxE8vQJX1duaqVlYR9tS/tYtKKdF388yNFUlnh2rFlKe9imtzHEBa0NdKzqoOOiTtrXLqf5oovY++VnGHG8ooSsqC1E7TEtvyNi09LawODxYZLDGVKJUbKJeFGBNTfQ8nM/aG5rL15DCylPSKTcvJ4fT/nJWCOBpp/IOMRHs4SjzUVafi4hKxSxfU0/YuW1/cSZ9LiErNyzc3p+XtMP2xX1/LBl5Yum5XT9wn8o5RKyYEx7D1tW2WYpOT3fksnpzJX+YZYmZM1XLX/qz5/eZ9Wdlp8jF70z89wDfE9EPgQcAm4FEJHFwAPGmJsYC4p5SUQ2B9d9OojU+ZKIXBbM+ADwkWoPrAmjryiKMruYyTpyz+0pxgxSJrDFGHMUuCnYrxgUY4x5/1SfqUZfURSlFMN0hWTOO9ToK4qijGPS0Ts1R00YfWfpCh7fGeeDb17O+r/7PI/b67j13k387unHySTidK95PdfccCmff/saVg1u4gfdMa647RL6PvyHnFh2DV/eeowfPP08B7e8TPyV3+FmkjS2dtPWdwl/+pPt7Np+kv49L5M4eRg3k8SORIl1LmbB0ovp7WvnstVdvHFlJ8+NpHENQWx+UFwtFqJzeStdF3fStnopbRevINy3Blm0klOZv8rH5kcsIWoLTfaYlt/eFCbaFaO5J0Z8YDRfXC2n5TvpZJGWnyPd0BrE5rsksyYfl5/T84fTvpY/kvL3Q43NWGG/AXqusJqv5duEwlYQo++POdnRoth8z8lgXLdoHsZzcZ1M0EClRM8PNPyw7WvyuXj7cKDpV9Pyc1SKzS/U4Avj9UuZyMkmImWLq1kl50yVqej5090oXbX8aWYao3fmGzVh9BVFUWYXfdNXFEWpH2YvemfWUaOvKIpSgsHk+z+cb6jRVxRFKUXf9OeWvQeO88Wf/yX7Xn0rb/n2i2x99OuMnDhA67K1XPnuW/jczeu4puEEJ77+pzzywK9453fuIvP6W/nWjgG+8Y3fsm/zAU7t30I2ESfc1Ep73yUsXnMh11y2mO/+y5OcOboXJzWCFYrkHbg9y3tYvbKDN13cw+uWtrKyvYHnGCuutiwWomdJi5+QtXox7WuXE+lbg71kNW77Us5YsbzTN1dcrT1s0xGxaI+EaFoYI9YVo6knRqynlcSBQ2MO3ILiauUckqeSLomsRyLjEk/7ztozacd36AYO3KFklpFUltGMSyjaXLa4Wj45K2xjhwTLtsimnbLF1fJJWQXO3ObGUMXiarZAyPY/c07dSsXVCsknZ9nli6vlxoAix265e1SiWkLWdCRT1aoDF9SJC/iO3GxmrmcxI9SE0VcURZldZic5ay5Qo68oilIOlXcURVHqBGOmq6DavKMmjH6osYkNe9aw8f/8fb5Ryvrb38/dG9ZxfdsIp/758zzxwHM8eyhOf9ol0XU9//DAC+zedIDBPZvIJuKEGpvpvOhyelev5KrXLOKWSxdx9dIWvvYXXy5qlNK1bCGrV3Vy3Zoe1i9pY2V7hObhI3gvvkhfLDKuUUr72uU0rFiDvdTX8uN2M/1JhyNnEjSHihuldDWEiHVFaVrYlNfyoz3tNPV2kt4yUFXLF8vGCkUYGHWIp7P5RikjGYd4Mkt8NMtwymEk7TCc8rX9TMalIdpQpOXnE7SCY8u2iASJVk4mXVXLN67/mWuiUk3Lt8XXniej5eewZXJavkxwj0pMRss/W+1dtfzzB43eURRFqReMwbhq9BVFUeoCYwxe1pnracwIavQVRVFKMeib/lxyyQUL+OX9/8iCpav5vQ98kM/cvI5rowOcfPBzPP7Av/P/jo1wKuPS3WBz89IF/NFf/yIflx9qbKZr9VUsWr2Cqy9bzM2X9HLV4mZaB3aSfuTxfFx+9wXdXLyqMx+Xv6Ktgab4IbwXX2Rkx1YGtu7hylXt+bj8ttUX0LByXT4uf8iK0T/q8MqZBIfiSfb1J1jcGKqo5Tct6iTa3U64swu7s5dM4umqWr5YNnY4wqF4clxc/nDKIZ7MMJpx81p+Nu3iZF0i0XDFuPxIQdG0WMTGLSnyVk7Lz21NYbuqlu/v+xo9VNfy82uWyWn5lshZOdymW8svvc903K8cquXPHmr0FUVR6gRjDJ7W01cURakfNHpHURSlXpil6B0R6QC+C/Th97i9zRhzusx5B4BhwAUcY8yVU7m+kHPpAa0oinJekoveqbZNA3cBTxhjVgFPBMeVeLMx5rKcwT+L64EaedM/tXUn73rgH/KdsfZ/5Y/54fe28etTSZKuoS8W5vqLO1lz2xUsfPd7OfG+f6axtZvO17yZC9Ys4frXLuYdaxdyaXcj4f2/YeR7j7Pr6S0c3Xicte+7h0sv6uS6VV1cvngBfQvChE/sIvvcRk5v38bg9v0M7hzk9L4hLv+vbyjqjOW2LaXfDdE/6nBwaJjD8ST7+xMcHExwfHCUuzqj+c5YTQubgkSsDqI97YTau7Haewh19uLF2nAzjxStWSw7v1nhCFbgzLVCYQ7Fk0WdsUZSQVJWysHJujgZz/8MtsamcEG3LMv/DFlEIzYN+a5XvkPXzSTznbFyzlugyIHrH3s0hOyizli2VbrvO3BzXbAKHa6VnK+5cdsaX2wNfAduzpl5tg5Ii/FO16JOWmd324r3K8dUnzETDlxQJ+5EeLPjyN0AXBfsfxN4Cvizmbxe3/QVRVFKCUI2q21Al4i8ULDdOcUnLTTGHAMIPnsqz4hfiMjGkmdM9vo8NfGmryiKMqtMXtMfKJFbxiEijwO9Zb66ewozusYYc1REeoDHRGSnMeaZKVyfR42+oihKCYbpi94xxlxf6TsROSEii4wxx0RkEXCywj2OBp8nReQhYD3wDDCp6wupCaOf8QwPtjzD5ts+yb0bj7M3kSFqC69pbeQ1b7yAi+94M+Hrbme36eTB7ce58NoNrHpVD++5YinXLm9jqTeIt+2nDDz4HEd+tZvjW06yZyTD0ZTDF259NWu7YnSbONbhX5N5aiNHX9rDwLbDDO4+zWB/giNJh9NZl7e/5z/hdVxAunkh/aMOJwazHBga4cCpUfb1J3jl1CjxoRSJMymSwxkWXdFLU08Lsd5OYj3tNHR1YHf2Yrf3YLV24UVbcRpb8Bpb82vN6/ihCGLb2IGOb4UiWOEIoUiUfScTjBRo+elMTr/3cAr2XcfDdT1a2qP5AmuRIi0/SMyy/fGGkIUTaPqFiViQ0/S9/D5ALOwnYYWDpCxfu/e1/LBl+bq8SF7XL7y2kHJjuWQuS4oTsWBMhz5bbVIK7l00XnLeVBOrplvHV+YQY/Ays1KG4SfAB4F7gs8fl54gIk2AZYwZDvbfCvzFZK8vRTV9RVGUUgx4nld1mwbuAW4Qkd3ADcExIrJYRB4OzlkIPCsiW4DngZ8bYx6Z6PqJqIk3fUVRlNnEMDtx+saYQeAtZcaPAjcF+/uA10zl+olQo68oilKKKe7lfD5RE0Z/0brl3P3er5J0DSubItx+xSLW3nYl3e9+Hye7L+WHe0/z7YcOsXf7Fgb2bueJ+/+YtW029p5fMfz9J9n17Esc23ScPccSHE1lOZVxcQ1ELOH37YNkfrOJoW3bGdy+n4Gdg5w+EOdI0qE/7XDG8Ui6Hq6Bk72X0z/qcOBA3C+qdtKPyR84nWRkKMXoSIZUIkNm+BSZ0ThLrlvnx+QHOr7d3o0Xa8NrbMVtbCFjRUhkPUYTTr6gWmlMvt0QxQpFsEMR7EgUKxzh4GCiYky+6xg8xx9zXQ/jGWJNkXEx+dGwndfx883NQ1a+gUppTH6htg/gea4fp18hJr9Qy88dT7bYGoxp+ZV0/Eq6/GSoFpM/3UXSVMuvRcx5W4ZhxjR9EfmGiJwUkW0FYx0i8piI7A4+22fq+YqiKGfN5OP0a46ZdOT+E3BjydiUU4YVRVFmG2MMbsaputUiM2b0g8SBUyXDG/BThQk+3zlTz1cURTl7TCBrTrzVIrOt6RelDAfZZWUJUo3vBFi2aCEQnZ0ZKoqiaOes2ccYcx9wH0DTktXm5nVt+YJqQxes57F9p/nOU4fZtf0J+ve8TOLkYdxMEjsSZcUjf8O+Z7dy5Plj7Ds6zOHkmPPWFmgN2yxsCLEsFuJ3/+uL+YJqh0ez45y34Dt8m0PCj3b2FxVUS5xJkziTLnLeOskR3EwKJ52k9fVvItTZi4kuwGtsJRttHXPepjyS2azf/SrlEI425523ViiC3RAtct7akSh2yO96NTCYLOu8dV0/IctzPVzH8TtguS49C5YXJWKNOXSLN1sEN5P0//tXcN7m//+4LrGwVdV5m+t8lXPETqbLlfFcbJFJOW/PpmDYZJ235TphncszlBrCgMkZgPOM2Tb6U04ZVhRFmW0MZraqbM46s52Rm0sZhkmmDCuKosw6Boxnqm61yIy96YvIt/HrPHeJyCvAZ/FThL8nIh8CDgG3ztTzFUVRzhZjwM1octaUMMbcUeGrKaUMAySHTtO3ZSP/tnuAHzx6iIM7HmbowEuMDh7FeC7hplZaFq+kY9lKevva+Kc/uZOjqSzxrP/nWcQSuhtCLG4MsaQ5Qseqdjou6qRj7XL+9bMPczrrEs+6JAs0vKgtRG2LBSGL1rBNd4PNV57e7xdTG8mQHE6QTcSLdHw3m/F19Fxi08VXk2lsJeUJiawhmfQYzWaIpxziaYeRjMNI2uFM2qGhtQsr5BdUy2n6VihCKGwTihQ3QBmJJ3EyvoZfpOUHzy5MsPKcDN0tjeN0/FwyVtiyCNu+Fh+2BM/J+v//Kuj4+X3PJRa2x2n4QJGObwmT0vNLv7NzTVNKdPxCmf1c/kydbg0fpqbjT3dTFG2GMs0Yo5q+oihKPeGp0VcURakTNGRTURSlfjCAV6OO2mqo0VcURSnFGHXkziW9Sxay/j9/tSgBK9q+kMVXvI2Fy9p49eou3nhRF1ctWUDfgjCf+ESa1rDN2pYGlsVCdC5vpeOidjrWLqPt4hWE+9Ygi1biti1lx8cfAnxnb2vYosm26IjYdERs2pvCRLtiNPfEaFrYxP7Nu8mmRsgm4vkErCLHbQFi2bzitZCMu/kErJGM78AdTjvER7OMpPz9kVSWWOeSogQs33FrEwpbWAVjdkg4uu/0uASswnkYz8XNHbsuPQsaihKwwpbf7crveuU7YnPVMl0nk19DqeO2EOO5NIascQlYhQ5XiwLHbomjsVqSll1wQblOWefidC1O7ip/n+mutKmO29rCaHKWoihKHaFGX1EUpZ7QjFxFUZT6YZYycifTY0RELhaRzQXbGRH5WPDd50TkSMF3N1V7Zk286fck+zkairD89W+lt6+Nq1d3c82FnVza08SiUAr72A4yO3/JqZ/uZPfOw/zBdcvzyVfNqy4i0rcGr6sPp20J/aMO/QmHA6dHObh/gL5YOJ981dLaQKwzSvPCJmI9zcR62on1dhDt7sBq7+H0Z7dMqOHnNivsd7p6/siZfPJVfDTLcMpPxhpJ+fujue5XWY8FXe355KtQ2A50fCuv8ec0+YaQxd5Ne4uSr7wCLd+4hR2v/P2eloa8lm9Zwaf4Gn7hviVjOv5kulxFbKso+aqwsFqu85W/LxXvUQnfJ1B4PCZin6veXk7HVw1fKcQwa3H6uR4j94jIXcHxnxXNxZhdwGUAImIDR4CHCk75sjHmbyb7wJow+oqiKLOKMXizE72zAb9cDfg9Rp6ixOiX8BZgrzHm4Nk+UOUdRVGUEozx3/SrbdNAUY8RoGKPkYDbgW+XjH1URLYGLWqrtqBVo68oilKGSXbO6hKRFwq2O0vvIyKPi8i2MtuGqcxHRCLALcD3C4a/BqzEl3+OAX9b7T41Ie8ceWWIjVvupNcaxT76MukdD3PqO7sY3PEKe3cO0n98hOMpl4GMw4jj8aUjT5JdsIj+UYcDiSz7h5Ic3DXKvv5dHBxIEB9K+YXThjN898YLaeppIdrTTrS7jejCbuz2buz2Hqy2brxoq781tOCkngN8/d4KRYr0+1zzEys8VjTtsR0nGUllGc24Rfq9kwman7hevnBa1+KWcfp9LGIXNT/JafpPjpyqqN/nWrgVjrc3hsvq92HLGtf8pJy/ovB+hUTssWJopfp9pQYok8Uu0zAFxhc1Oxstvto1ZyufT7eOr8whZtJv8gPGmCsnvpW5vtJ3IjKVHiNvBzYZY04U3Du/LyL3Az+rNmF901cURSkliNOvtk0DU+kxcgcl0k7wiyLHu4Bt1R5YE2/6iqIos4lh1gqule0xIiKLgQeMMTcFxzHgBuAjJdd/SUQuC6Z8oMz341CjryiKUooxuJmZN/rGmEHK9BgxxhwFbio4HgU6y5z3/qk+U42+oihKCcaAZ7QMw5zRvaCBXde8iaf7RzmecjmddRlxPDJBRpwtfsG05pDF4sYwf/TLIQ4OHCFxJs3omTSjw2nSCb9QWiYRx0kl8JwMTjrJJfd9AlnQhRdtxTS24DYuYCTrkch6JB2PZNYjfsohnh6msbU777C1G6KBAzeCHYkGDtyGgsQqm627+vEcDyfrd7byHbcuxph8p6tcl6vXXbWUSMgiGrbzXa5y3a3yW1AkLZuIl3XYwlinq8JiaV2xyDiHbe64tDCaV1BwbSKM5xK2pGISVblOV1PBLrluujtdzbXLVX2+8x9Xjb6iKEp9YIDztN6aGn1FUZRy6Ju+oihKneAZ8vLx+UZNGH1v2YU8vOMUzSGLBSGblU1hOiI2LZ0xYl1RmhY20dTTQqy3k1hPO30P/DDf5CRXlKyUXHG0bV3riacc4qccRtIZ4unjjBQUSIsnsyQzDsMph+4164s0ezskRQ1PLDs4DllEIzZbf70/r9nn5mE8t2yBtNcuf3Nesw/b4idOCYRs/9Mf9/ezqcSEDU5KxzqiYX/NQTOT0gJpef29wvWViNhSpE1PZ4E0f54z0+Ck3OVaIE0pReUdRVGUOsFgVN5RFEWpF9SRqyiKUmeo0Z9Ddh88wYs/+h/5Qmg0tftF0BoXkA1FGc16JB3DUNbjSMYl8+BnscMRIk2tZQuh2Q3+ZygS5k++v5VsurAAml8UzQvi6l3Hyzchv3T9srKF0BoKY+kLYuyf/cEjBXH0Y3H1hXp5Lq7+VT0tWMK4OPpycfVuOpm/fjLae3PEV9snKoKW08mn0ugkUhBMPx2F0AqxS24wnRL5TBVGUx3//MEYjd5RFEWpGwwavaMoilI3qKavKIpSZ6i8oyiKUif4mv5cz2JmqAmjb0caueP4FYwcCLpPZU7hZPuDTlQurmOCwma+M/bqO24lFCRIjTlZbaJBV6rCgmb/+8s/AAo7T405XkuLmX3442/yk6Ssse5TEzlek6dPjFtLJUfphe2NgO+wrNZ9arJF0XLEwlaRY7V8ctKUbgkUO3JLOVefpj2DXlF1uCqTQd/0FUVR6gQDzEoLlTlAjb6iKEoJBqPRO4qiKPWCH72jRn/OuGR5Bw9/9b5Jn//SvX8/6XO/+Km9kz73hgvbJn0uTE17X9QcntK9p0IuOWu6CZ1rBtYEqO6uzCnnsSN3ZqxBFUTkRhHZJSJ7ROSuuZiDoihKJXJv+tW2c0VEbhWR7SLiiciVE5xX1maKSIeIPCYiu4PP9mrPnHWjLyI28FXg7cA64A4RWTfb81AURZkI11TfpoFtwLuBZyqdUMVm3gU8YYxZBTwRHE/IXLzprwf2GGP2GWMywHeADXMwD0VRlLJ4+GUYqm3nijFmhzFmV5XTJrKZG4BvBvvfBN5Z7ZliZtlZISLvAW40xvxhcPx+4HXGmI+WnHcncGdweAn+b8TzhS5gYK4nMc2cb2vS9cx/Kq1puTGm+1xuLCKPBPevRiOQKji+zxgzeQfk2POeAj5pjHmhzHcVbaaIDBlj2grOPW2MmVDimQtHbjkX3bjfPMF/uPsAROQFY0xFvavWON/WA+ffmnQ985+ZXJMx5sbpupeIPA70lvnqbmPMjydzizJjZ/22PhdG/xXggoLjpcDROZiHoijKjGOMuf4cbzGRzTwhIouMMcdEZBFwstrN5kLT/y2wSkRWiEgEuB34yRzMQ1EUpRaYyGb+BPhgsP9BoOpfDrNu9I0xDvBR4FFgB/A9Y8z2KpdNWSOb55xv64Hzb026nvlPza9JRN4lIq8AVwM/F5FHg/HFIvIwVLWZ9wA3iMhu4IbgeOJnzrYjV1EURZk75iQ5S1EURZkb1OgriqLUEfPa6NdquQYR+YaInBSRbQVjFdOlReTPgzXuEpG3zc2sKyMiF4jIL0VkR5Ay/t+D8Zpck4g0isjzIrIlWM/ng/GaXE8OEbFF5EUR+VlwXOvrOSAiL4nIZhF5IRir6TXNC4wx83IDbGAvcCEQAbYA6+Z6XpOc+7XA5cC2grEvAXcF+3cBfxXsrwvW1gCsCNZsz/UaStazCLg82G8BfhfMuybXhB/33Bzsh4HfAK+v1fUUrOvjwL8CP6v1n7lgngeArpKxml7TfNjm85t+zZZrMMY8A5wqGa6ULr0B+I4xJm2M2Q/swV/7vMEYc8wYsynYH8aPIFhCja7J+IwEh+FgM9ToegBEZCnwH4AHCoZrdj0TcD6uaVaZz0Z/CXC44PiVYKxWWWiMOQa+EQV6gvGaWqeI9AGvxX87rtk1BVLIZvxklseMMTW9HuArwKcobvhUy+sB/xfxL0RkY1CWBWp/TXPOfK6nP62px/OYmlmniDQDPwQ+Zow5I5WL3s/7NRljXOAyEWkDHhKRSyY4fV6vR0TeAZw0xmwUkesmc0mZsXmzngKuMcYcFZEe4DER2TnBubWypjlnPr/pn2/lGk4EadKUpEvXxDpFJIxv8L9ljPm3YLim1wRgjBkCngJupHbXcw1wi4gcwJdBf19E/oXaXQ8AxpijwedJ4CF8uaam1zQfmM9G/3wr11ApXfonwO0i0iAiK4BVwPNzML+KiP9K/4/ADmPMvQVf1eSaRKQ7eMNHRKLA9cBOanQ9xpg/N8YsNcb04f87edIY8wfU6HoARKRJRFpy+8Bb8Svt1uya5g1z7UmeaANuwo8U2YtfkW7O5zTJeX8bOAZk8d9APgR04jc52B18dhScf3ewxl3A2+d6/mXW8wb8P5W3ApuD7aZaXRPwauDFYD3bgM8E4zW5npK1XcdY9E7Nrgc/am9LsG3P/fuv5TXNl03LMCiKotQR81neURRFUaYZNfqKoih1hBp9RVGUOkKNvqIoSh2hRl9RFKWOUKOvzDki4gaVFLcHlS8/LiJn/bMpIp8u2O8rrHaqKPWOGn1lPpA0xlxmjHkVfsu3m4DPnsP9Pl39FEWpT9ToK/MK46fc3wl8VHxsEflrEfmtiGwVkY8AiMh1IvKMiDwkIi+LyNdFxBKRe4Bo8JfDt4Lb2iJyf/CXxC+CLFxFqUvU6CvzDmPMPvyfzR78bOa4MeYq4Crgw0GaPfi1WD4BXAqsBN5tjLmLsb8c3hectwr4avCXxBDwH2dvNYoyv1Cjr8xXclUT3wp8ICiD/Bv8NPxVwXfPG7/fgotf+uINFe613xizOdjfCPTNzJQVZf4zn0srK3WKiFwIuPgVFAX4b8aYR0vOuY7xpXMr1RRJF+y7gMo7St2ib/rKvEJEuoGvA39n/MJQjwL/JSjtjIisDqouAqwPqrBawHuBZ4PxbO58RVGK0Td9ZT4QDeSbMOAA/xfIlXB+AF+O2RSUeO5nrEXer4B78DX9Z/BrrgPcB2wVkU34lRcVRQnQKptKTRLIO580xrxjrueiKLWEyjuKoih1hL7pK4qi1BH6pq8oilJHqNFXFEWpI9ToK4qi1BFq9BVFUeoINfqKoih1xP8HFTtQ3rYLFVUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap=\"RdBu\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel(\"Position\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遮挡(Masking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遮挡一批序列中所有的填充标记（pad tokens）。这确保模型不会将填充作为输入。该 mask 表明填充值 0 出现的位置：在这些位置 mask 输出 1，否则输出 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    # 添加额外的维度来将填充加到 注意力对数（logits）\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=207718, shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前瞻遮挡（look-ahead mask）用于遮挡一个序列中的后续标记（future tokens）。换句话说，该 mask 表明了不应该使用的条目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这意味着要预测第三个词，将仅使用第一个和第二个词。与此类似，预测第四个词，仅使用第一个，第二个和第三个词，依此类推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.linalg.band_part  https://blog.csdn.net/weixin_44815943/article/details/104375784?\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=207733, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1,3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按比缩放的点积注意力(Scaled dot product attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dot-Product](https://tensorflow.google.cn/images/tutorials/transformer/scaled_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 使用的注意力函数有三个输入：Q（请求（query））、K（主键（key））、V（数值（value））。用于计算注意力权重的等式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点积注意力被缩小了深度的平方根倍。这样做是因为对于较大的深度值，点积的大小会增大，从而推动 softmax 函数往仅有很小的梯度的方向靠拢，导致了一种很硬的（hard）softmax。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为 dk。因此，dk 的平方根被用于缩放（而非其他数值），因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样会获得一个更平缓的 softmax。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遮挡（mask）与 -1e9（接近于负无穷）相乘。这样做是因为遮挡与缩放的 Q 和 K 的矩阵乘积相加，并在 softmax 之前立即应用。目标是将这些单元归零，因为 softmax 的较大负数输入在输出中接近于零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_prodcut_attention(q, k, v, mask):\n",
    "    \"\"\"计算注意力权重。\n",
    "      q, k, v 必须具有匹配的前置维度。\n",
    "      k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "      虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "      但是 mask 必须能进行广播转换以便求和。\n",
    "      \n",
    "      参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)\n",
    "        mask: Float 张量，其形状能转换成\n",
    "          (..., seq_len_q, seq_len_k)。默认为None。\n",
    "    \n",
    "      返回值:\n",
    "        输出，注意力权重\n",
    "      \"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) # (.., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 缩放 matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # 将mask加入到缩放的张量上\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9) # 乘以-1e9\n",
    "    \n",
    "    # softmax 在最后一个轴(seq_len_k)上归一化，因此分数相加等于1\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=1) # # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 softmax 在 K 上进行归一化后，它的值决定了分配到 Q 的重要程度。\n",
    "\n",
    "输出表示注意力权重和 V（数值）向量的乘积。这确保了要关注的词保持原样，而无关的词将被清除掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_prodcut_attention(\n",
    "        q, k, v, None)\n",
    "    print(\"Attention weights are:\")\n",
    "    print(temp_attn)\n",
    "    print(\"Output is :\")\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is :\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# 这条请求query符合第二个主键key， 因此返回了第二个value\n",
    "temp_q = tf.constant([[0,10,0]], dtype=tf.float32) # (1 ,3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is :\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 这条请求符合重复出现的主键（第三第四个），\n",
    "# 因此，对所有的相关数值取了平均\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is :\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 这条请求符合第一和第二条主键，\n",
    "# 因此，对它们的数值去了平均。\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将所有请求一起传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is :\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力（Multi-head attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi](https://tensorflow.google.cn/images/tutorials/transformer/multi_head_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多头注意力由四部分组成：\n",
    "\n",
    "+ 线性层并分拆成多头。\n",
    "+ 按比缩放的点积注意力。\n",
    "+ 多头及联。\n",
    "+ 最后一层线性层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个多头注意力块有三个输入：Q（请求）、K（主键）、V（数值）。这些输入经过线性（Dense）层，并分拆成多头。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面定义的 scaled_dot_product_attention 函数应用于每个头（进行了广播（broadcasted）以提高效率）。注意力这步必须使用一个恰当的 mask。然后将每个头的注意力输出连接起来（用tf.transpose 和 tf.reshape），并放入最后的 Dense 层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q、K、和 V 被拆分到了多个头，而非单个的注意力头，因为多头允许模型共同注意来自不同表示空间的不同位置的信息。在分拆后，每个头部的维度减少，因此总的计算成本与有着全部维度的单个注意力头相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"分拆最后一个维度到 (num_heads, depth).\n",
    "            转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0,2,1,3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q) # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k) # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v) # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 分拆成multihead\n",
    "        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_prodcut_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3]) # (batch_size, seq_len_q, num_heads, depth)\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model)) # # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个 MultiHeadAttention 层进行尝试。在序列中的每个位置 y，MultiHeadAttention 在序列中的所有其他位置运行所有8个注意力头，在每个位置y，返回一个新的同样长度的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512)) # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 点式前馈网络（Point wise feed forward network）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点式前馈网络由两层全联接层组成，两层之间有一个 ReLU 激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation=\"relu\"), # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model) # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64,50,512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码与解码（Encoder and decoder）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoder_Decoder](https://tensorflow.google.cn/images/tutorials/transformer/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 模型与标准的具有注意力机制的序列到序列模型（sequence to sequence with attention model），遵循相同的一般模式:\n",
    "+ 输入语句经过 N 个编码器层，为序列中的每个词/标记生成一个输出。\n",
    "+ 解码器关注编码器的输出以及它自身的输入（自注意力）来预测下一个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器层（Encoder layer）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个编码器层包括以下子层：\n",
    "\n",
    "+ 多头注意力（有填充遮挡）\n",
    "+ 点式前馈网络（Point wise feed forward networks）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个子层在其周围有一个残差连接，然后进行层归一化。残差连接有助于避免深度网络中的梯度消失问题。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。Transformer 中有 N 个编码器层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # 子层\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        # Layernorm\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        # Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        # MultiHeadAttention\n",
    "        attn_output, _ = self.mha(x, x, x, mask) # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output) # (batch_size, input_seq_len, d_model)\n",
    "        # Feedforward\n",
    "        ffn_output = self.ffn(out1) # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output) # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None\n",
    ")\n",
    "sample_encoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个解码器层包括以下子层：\n",
    "+ 遮挡的多头注意力（前瞻遮挡和填充遮挡）\n",
    "+ 多头注意力（用填充遮挡）。V（数值）和 K（主键）接收编码器输出作为输入。Q（请求）接收遮挡的多头注意力子层的输出。\n",
    "+ 点式前馈网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个子层在其周围有一个残差连接，然后进行层归一化。每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "Transformer 中共有 N 个解码器层。  \n",
    "当 Q 接收到解码器的第一个注意力块的输出，并且 K 接收到编码器的输出时，注意力权重表示根据编码器的输出赋予解码器输入的重要性。换一种说法，解码器通过查看编码器输出和对其自身输出的自注意力，预测下一个词。参看按比缩放的点积注意力部分的演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # MultiheadAttention\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # FeedForward\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        # Layer Norm\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # look_ahead_mask预测时当前的词只与前面出现的有关，而出后面的无关\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        # MultiheadAttention, padding_mask: 填充遮挡\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask # (batch_size, target_seq_len, d_model)\n",
    "        )\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1) # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        # FeedForward\n",
    "        ffn_output = self.ffn(out2) # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None\n",
    ")\n",
    "sample_decoder_layer_output.shape # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器（Encoder）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码器 包括：\n",
    "\n",
    "+ 输入嵌入（Input Embedding）\n",
    "+ 位置编码（Positional Encoding）\n",
    "+ N 个编码器层（encoder layers）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入经过嵌入（embedding）后，该嵌入与位置编码相加。该加法结果的输出是编码器层的输入。编码器的输出是解码器的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Word_Embedding and Positinal Embedding\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        \n",
    "        \n",
    "        # 编码层\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                          for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # 将嵌入和位置编码相加\n",
    "        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        \n",
    "        return x # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                        dff=2048, input_vocab_size=8500, \n",
    "                        maximum_position_encoding=10000)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)),\n",
    "                                      training=False, mask=None)\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器(Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器包括：\n",
    "\n",
    "+ 输出嵌入（Output Embedding）\n",
    "+ 位置编码（Positional Encoding）\n",
    "+ N 个解码器层（decoder layers）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标（target）经过一个嵌入后，该嵌入和位置编码相加。该加法结果是解码器层的输入。解码器的输出是最后的线性层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # WordEmbedding and PostionalEmbedding\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        # 解码层\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                          for _ in range(num_layers)]\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training,\n",
    "            look_ahead_mask, padding_mask):\n",
    "        \n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # Embedding Layer\n",
    "        x = self.embedding(x) # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Decoder Layers\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                  look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[\"decoder_layer{}_block1\".format(i+1)] = block1\n",
    "            attention_weights[\"decoder_layer{}_block2\".format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "\n",
    "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False, look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn[\"decoder_layer1_block2\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 包括编码器，解码器和最后的线性层。解码器的输出是线性层的输入，返回线性层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "        # 解码器\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "        # final 线性层\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "        \n",
    "        # 编码器输出作为解码器输入\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        \n",
    "        # 解码器输出\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        # 线性层输出\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 8000])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置超参数（hyperparameters）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让本示例小且相对较快，已经减小了num_layers、 d_model 和 dff 的值。\n",
    "\n",
    "Transformer 的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器（Optimizer）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据论文中的公式，将 Adam 优化器与自定义的学习速率调度程序（scheduler）配合使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ lrate = d_{model}^{-0.5} * min(step\\_num^{-0.5}, step_num * warmup\\_steps^{-1.5}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.8, beta_2=0.98,\n",
    "                                    epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Train Step')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数与指标（Loss and metrics）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于目标序列是填充（padded）过的，因此在计算损失函数时，应用填充遮挡非常重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "from_logits=True, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ * mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练与检查点（Training and checkpointing）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # 编码器填充遮挡\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # 在解码器的第二个注意力模块使用。\n",
    "    # 该填充遮挡用于遮挡编码器的输出。\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # 在解码器的第一个注意力模块使用。\n",
    "    # 用于填充（pad）和遮挡（mask）解码器获取到的输入的后续标记（future tokens）。\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建检查点的路径和检查点管理器（manager）。这将用于在每 n 个周期（epochs）保存检查点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标（target）被分成了 tar_inp 和 tar_real。tar_inp 作为输入传递到解码器。tar_real 是位移了 1 的同一个输入：在 tar_inp 中的每个位置，tar_real 包含了应该被预测到的下一个标记（token）。\n",
    "\n",
    "例如，sentence = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "tar_inp = \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "tar_real = \"A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "Transformer 是一个自回归（auto-regressive）模型：它一次作一个部分的预测，然后使用到目前为止的自身的输出来决定下一步要做什么。\n",
    "\n",
    "在训练过程中，本示例使用了 teacher-forcing 的方法（就像文本生成教程中一样）。无论模型在当前时间步骤下预测出什么，teacher-forcing 方法都会将真实的输出传递到下一个时间步骤上。\n",
    "\n",
    "当 transformer 预测每个词时，自注意力（self-attention）功能使它能够查看输入序列中前面的单词，从而更好地预测下一个单词。 \n",
    "\n",
    "为了防止模型在期望的输出上达到峰值，模型使用了前瞻遮挡（look-ahead mask）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该 @tf.function 将追踪-编译 train_step 到 TF 图中，以便更快地\n",
    "# 执行。该函数专用于参数张量的精确形状。为了避免由于可变序列长度或可变\n",
    "# 批次大小（最后一批次较小）导致的再追踪，使用 input_signature 指定\n",
    "# 更多的通用形状。\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "葡萄牙语作为输入语言，英语为目标语言。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.0734 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.8985 Accuracy 0.0409\n",
      "Epoch 1 Batch 100 Loss 8.5705 Accuracy 0.2807\n",
      "Epoch 1 Batch 150 Loss 8.2698 Accuracy 0.3644\n",
      "Epoch 1 Batch 200 Loss 7.9949 Accuracy 0.4046\n",
      "Epoch 1 Batch 250 Loss 7.7061 Accuracy 0.4298\n",
      "Epoch 1 Batch 300 Loss 7.3980 Accuracy 0.4458\n",
      "Epoch 1 Batch 350 Loss 7.0719 Accuracy 0.4573\n",
      "Epoch 1 Batch 400 Loss 6.7470 Accuracy 0.4652\n",
      "Epoch 1 Batch 450 Loss 6.4342 Accuracy 0.4728\n",
      "Epoch 1 Batch 500 Loss 6.1516 Accuracy 0.4782\n",
      "Epoch 1 Batch 550 Loss 5.9008 Accuracy 0.4830\n",
      "Epoch 1 Batch 600 Loss 5.6870 Accuracy 0.4873\n",
      "Epoch 1 Batch 650 Loss 5.4976 Accuracy 0.4925\n",
      "Epoch 1 Batch 700 Loss 5.3332 Accuracy 0.4965\n",
      "Epoch 1 Loss 5.3271 Accuracy 0.4966\n",
      "Time taken for 1 epoch: 147.93480587005615 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.2334 Accuracy 0.5252\n",
      "Epoch 2 Batch 50 Loss 3.0823 Accuracy 0.5588\n",
      "Epoch 2 Batch 100 Loss 3.1335 Accuracy 0.5599\n",
      "Epoch 2 Batch 150 Loss 3.1331 Accuracy 0.5615\n",
      "Epoch 2 Batch 200 Loss 3.2138 Accuracy 0.5565\n",
      "Epoch 2 Batch 250 Loss 3.2468 Accuracy 0.5524\n",
      "Epoch 2 Batch 300 Loss 3.2782 Accuracy 0.5480\n",
      "Epoch 2 Batch 350 Loss 3.3038 Accuracy 0.5447\n",
      "Epoch 2 Batch 400 Loss 3.3083 Accuracy 0.5442\n",
      "Epoch 2 Batch 450 Loss 3.3143 Accuracy 0.5434\n",
      "Epoch 2 Batch 500 Loss 3.3192 Accuracy 0.5426\n",
      "Epoch 2 Batch 550 Loss 3.3158 Accuracy 0.5427\n",
      "Epoch 2 Batch 600 Loss 3.3295 Accuracy 0.5415\n",
      "Epoch 2 Batch 650 Loss 3.3275 Accuracy 0.5416\n",
      "Epoch 2 Batch 700 Loss 3.3273 Accuracy 0.5414\n",
      "Epoch 2 Loss 3.3274 Accuracy 0.5413\n",
      "Time taken for 1 epoch: 118.45687699317932 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.4078 Accuracy 0.5401\n",
      "Epoch 3 Batch 50 Loss 3.2598 Accuracy 0.5490\n",
      "Epoch 3 Batch 100 Loss 3.3009 Accuracy 0.5425\n",
      "Epoch 3 Batch 150 Loss 3.3082 Accuracy 0.5418\n",
      "Epoch 3 Batch 200 Loss 3.3080 Accuracy 0.5414\n",
      "Epoch 3 Batch 250 Loss 3.3213 Accuracy 0.5399\n",
      "Epoch 3 Batch 300 Loss 3.3112 Accuracy 0.5407\n",
      "Epoch 3 Batch 350 Loss 3.3196 Accuracy 0.5398\n",
      "Epoch 3 Batch 400 Loss 3.3251 Accuracy 0.5393\n",
      "Epoch 3 Batch 450 Loss 3.3247 Accuracy 0.5395\n",
      "Epoch 3 Batch 500 Loss 3.3253 Accuracy 0.5391\n",
      "Epoch 3 Batch 550 Loss 3.3192 Accuracy 0.5400\n",
      "Epoch 3 Batch 600 Loss 3.3165 Accuracy 0.5401\n",
      "Epoch 3 Batch 650 Loss 3.3437 Accuracy 0.5397\n",
      "Epoch 3 Batch 700 Loss 3.3537 Accuracy 0.5395\n",
      "Epoch 3 Loss 3.3538 Accuracy 0.5395\n",
      "Time taken for 1 epoch: 118.20843482017517 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.8091 Accuracy 0.4859\n",
      "Epoch 4 Batch 50 Loss 3.4581 Accuracy 0.5365\n",
      "Epoch 4 Batch 100 Loss 3.4475 Accuracy 0.5385\n",
      "Epoch 4 Batch 150 Loss 3.4688 Accuracy 0.5360\n",
      "Epoch 4 Batch 200 Loss 3.4860 Accuracy 0.5349\n",
      "Epoch 4 Batch 250 Loss 3.4812 Accuracy 0.5355\n",
      "Epoch 4 Batch 300 Loss 3.4895 Accuracy 0.5343\n",
      "Epoch 4 Batch 350 Loss 3.4829 Accuracy 0.5338\n",
      "Epoch 4 Batch 400 Loss 3.4801 Accuracy 0.5334\n",
      "Epoch 4 Batch 450 Loss 3.4666 Accuracy 0.5345\n",
      "Epoch 4 Batch 500 Loss 3.4564 Accuracy 0.5347\n",
      "Epoch 4 Batch 550 Loss 3.4388 Accuracy 0.5357\n",
      "Epoch 4 Batch 600 Loss 3.4389 Accuracy 0.5354\n",
      "Epoch 4 Batch 650 Loss 3.4297 Accuracy 0.5356\n",
      "Epoch 4 Batch 700 Loss 3.4230 Accuracy 0.5358\n",
      "Epoch 4 Loss 3.4232 Accuracy 0.5358\n",
      "Time taken for 1 epoch: 118.73977589607239 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.5488 Accuracy 0.5058\n",
      "Epoch 5 Batch 50 Loss 3.3342 Accuracy 0.5387\n",
      "Epoch 5 Batch 100 Loss 3.3147 Accuracy 0.5388\n",
      "Epoch 5 Batch 150 Loss 3.3251 Accuracy 0.5375\n",
      "Epoch 5 Batch 200 Loss 3.3038 Accuracy 0.5395\n",
      "Epoch 5 Batch 250 Loss 3.2942 Accuracy 0.5408\n",
      "Epoch 5 Batch 300 Loss 3.2982 Accuracy 0.5404\n",
      "Epoch 5 Batch 350 Loss 3.3019 Accuracy 0.5398\n",
      "Epoch 5 Batch 400 Loss 3.3044 Accuracy 0.5399\n",
      "Epoch 5 Batch 450 Loss 3.3101 Accuracy 0.5395\n",
      "Epoch 5 Batch 500 Loss 3.3124 Accuracy 0.5391\n",
      "Epoch 5 Batch 550 Loss 3.3074 Accuracy 0.5395\n",
      "Epoch 5 Batch 600 Loss 3.3141 Accuracy 0.5393\n",
      "Epoch 5 Batch 650 Loss 3.3148 Accuracy 0.5388\n",
      "Epoch 5 Batch 700 Loss 3.3108 Accuracy 0.5389\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train\\ckpt-1\n",
      "Epoch 5 Loss 3.3119 Accuracy 0.5388\n",
      "Time taken for 1 epoch: 119.45859217643738 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.1598 Accuracy 0.5477\n",
      "Epoch 6 Batch 50 Loss 3.2327 Accuracy 0.5470\n",
      "Epoch 6 Batch 100 Loss 3.2476 Accuracy 0.5440\n",
      "Epoch 6 Batch 150 Loss 3.2448 Accuracy 0.5438\n",
      "Epoch 6 Batch 200 Loss 3.2410 Accuracy 0.5440\n",
      "Epoch 6 Batch 250 Loss 3.2420 Accuracy 0.5443\n",
      "Epoch 6 Batch 300 Loss 3.2591 Accuracy 0.5422\n",
      "Epoch 6 Batch 350 Loss 3.2768 Accuracy 0.5409\n",
      "Epoch 6 Batch 400 Loss 3.2767 Accuracy 0.5410\n",
      "Epoch 6 Batch 450 Loss 3.2834 Accuracy 0.5406\n",
      "Epoch 6 Batch 500 Loss 3.2845 Accuracy 0.5406\n",
      "Epoch 6 Batch 550 Loss 3.2831 Accuracy 0.5404\n",
      "Epoch 6 Batch 600 Loss 3.2814 Accuracy 0.5405\n",
      "Epoch 6 Batch 650 Loss 3.2786 Accuracy 0.5408\n",
      "Epoch 6 Batch 700 Loss 3.2816 Accuracy 0.5404\n",
      "Epoch 6 Loss 3.2820 Accuracy 0.5403\n",
      "Time taken for 1 epoch: 118.03529953956604 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.7513 Accuracy 0.4754\n",
      "Epoch 7 Batch 50 Loss 3.2428 Accuracy 0.5441\n",
      "Epoch 7 Batch 100 Loss 3.2407 Accuracy 0.5437\n",
      "Epoch 7 Batch 150 Loss 3.2320 Accuracy 0.5445\n",
      "Epoch 7 Batch 200 Loss 3.2555 Accuracy 0.5413\n",
      "Epoch 7 Batch 250 Loss 3.2888 Accuracy 0.5399\n",
      "Epoch 7 Batch 300 Loss 3.3097 Accuracy 0.5396\n",
      "Epoch 7 Batch 350 Loss 3.3084 Accuracy 0.5402\n",
      "Epoch 7 Batch 400 Loss 3.3191 Accuracy 0.5393\n",
      "Epoch 7 Batch 450 Loss 3.2956 Accuracy 0.5410\n",
      "Epoch 7 Batch 500 Loss 3.2770 Accuracy 0.5427\n",
      "Epoch 7 Batch 550 Loss 3.2542 Accuracy 0.5451\n",
      "Epoch 7 Batch 600 Loss 3.2291 Accuracy 0.5483\n",
      "Epoch 7 Batch 650 Loss 3.2050 Accuracy 0.5518\n",
      "Epoch 7 Batch 700 Loss 3.1894 Accuracy 0.5541\n",
      "Epoch 7 Loss 3.1889 Accuracy 0.5541\n",
      "Time taken for 1 epoch: 118.11517262458801 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.9444 Accuracy 0.5857\n",
      "Epoch 8 Batch 50 Loss 2.9335 Accuracy 0.5873\n",
      "Epoch 8 Batch 100 Loss 2.9069 Accuracy 0.5911\n",
      "Epoch 8 Batch 150 Loss 2.8821 Accuracy 0.5945\n",
      "Epoch 8 Batch 200 Loss 2.8781 Accuracy 0.5945\n",
      "Epoch 8 Batch 250 Loss 2.8668 Accuracy 0.5959\n",
      "Epoch 8 Batch 300 Loss 2.8671 Accuracy 0.5957\n",
      "Epoch 8 Batch 350 Loss 2.8598 Accuracy 0.5965\n",
      "Epoch 8 Batch 400 Loss 2.8574 Accuracy 0.5966\n",
      "Epoch 8 Batch 450 Loss 2.8505 Accuracy 0.5971\n",
      "Epoch 8 Batch 500 Loss 2.8463 Accuracy 0.5973\n",
      "Epoch 8 Batch 550 Loss 2.8449 Accuracy 0.5971\n",
      "Epoch 8 Batch 600 Loss 2.8430 Accuracy 0.5972\n",
      "Epoch 8 Batch 650 Loss 2.8398 Accuracy 0.5975\n",
      "Epoch 8 Batch 700 Loss 2.8373 Accuracy 0.5976\n",
      "Epoch 8 Loss 2.8366 Accuracy 0.5976\n",
      "Time taken for 1 epoch: 118.02369165420532 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.5331 Accuracy 0.6342\n",
      "Epoch 9 Batch 50 Loss 2.7471 Accuracy 0.6065\n",
      "Epoch 9 Batch 100 Loss 2.7621 Accuracy 0.6033\n",
      "Epoch 9 Batch 150 Loss 2.7838 Accuracy 0.6008\n",
      "Epoch 9 Batch 200 Loss 2.7767 Accuracy 0.6010\n",
      "Epoch 9 Batch 250 Loss 2.7831 Accuracy 0.6006\n",
      "Epoch 9 Batch 300 Loss 2.7880 Accuracy 0.6004\n",
      "Epoch 9 Batch 350 Loss 2.7829 Accuracy 0.6008\n",
      "Epoch 9 Batch 400 Loss 2.7787 Accuracy 0.6011\n",
      "Epoch 9 Batch 450 Loss 2.7779 Accuracy 0.6015\n",
      "Epoch 9 Batch 500 Loss 2.7799 Accuracy 0.6012\n",
      "Epoch 9 Batch 550 Loss 2.7783 Accuracy 0.6015\n",
      "Epoch 9 Batch 600 Loss 2.7780 Accuracy 0.6016\n",
      "Epoch 9 Batch 650 Loss 2.7873 Accuracy 0.6007\n",
      "Epoch 9 Batch 700 Loss 2.7950 Accuracy 0.5999\n",
      "Epoch 9 Loss 2.7958 Accuracy 0.5998\n",
      "Time taken for 1 epoch: 118.13956642150879 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 3.2967 Accuracy 0.5361\n",
      "Epoch 10 Batch 50 Loss 3.1076 Accuracy 0.5727\n",
      "Epoch 10 Batch 100 Loss 3.2904 Accuracy 0.5526\n",
      "Epoch 10 Batch 150 Loss 3.2960 Accuracy 0.5492\n",
      "Epoch 10 Batch 200 Loss 3.3207 Accuracy 0.5450\n",
      "Epoch 10 Batch 250 Loss 3.3156 Accuracy 0.5443\n",
      "Epoch 10 Batch 300 Loss 3.3057 Accuracy 0.5448\n",
      "Epoch 10 Batch 350 Loss 3.2986 Accuracy 0.5451\n",
      "Epoch 10 Batch 400 Loss 3.3024 Accuracy 0.5438\n",
      "Epoch 10 Batch 450 Loss 3.2973 Accuracy 0.5439\n",
      "Epoch 10 Batch 500 Loss 3.2909 Accuracy 0.5439\n",
      "Epoch 10 Batch 550 Loss 3.2910 Accuracy 0.5435\n",
      "Epoch 10 Batch 600 Loss 3.2916 Accuracy 0.5431\n",
      "Epoch 10 Batch 650 Loss 3.2878 Accuracy 0.5433\n",
      "Epoch 10 Batch 700 Loss 3.2859 Accuracy 0.5432\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train\\ckpt-2\n",
      "Epoch 10 Loss 3.2854 Accuracy 0.5432\n",
      "Time taken for 1 epoch: 119.7583417892456 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 3.3251 Accuracy 0.5452\n",
      "Epoch 11 Batch 50 Loss 3.1832 Accuracy 0.5494\n",
      "Epoch 11 Batch 100 Loss 3.2121 Accuracy 0.5462\n",
      "Epoch 11 Batch 150 Loss 3.2125 Accuracy 0.5454\n",
      "Epoch 11 Batch 200 Loss 3.1992 Accuracy 0.5470\n",
      "Epoch 11 Batch 250 Loss 3.2006 Accuracy 0.5471\n",
      "Epoch 11 Batch 300 Loss 3.2138 Accuracy 0.5464\n",
      "Epoch 11 Batch 350 Loss 3.2259 Accuracy 0.5454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 400 Loss 3.2324 Accuracy 0.5448\n",
      "Epoch 11 Batch 450 Loss 3.2418 Accuracy 0.5447\n",
      "Epoch 11 Batch 500 Loss 3.2652 Accuracy 0.5441\n",
      "Epoch 11 Batch 550 Loss 3.2841 Accuracy 0.5434\n",
      "Epoch 11 Batch 600 Loss 3.2916 Accuracy 0.5426\n",
      "Epoch 11 Batch 650 Loss 3.2924 Accuracy 0.5425\n",
      "Epoch 11 Batch 700 Loss 3.3112 Accuracy 0.5422\n",
      "Epoch 11 Loss 3.3121 Accuracy 0.5422\n",
      "Time taken for 1 epoch: 118.23062705993652 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 3.7672 Accuracy 0.5165\n",
      "Epoch 12 Batch 50 Loss 3.6473 Accuracy 0.5310\n",
      "Epoch 12 Batch 100 Loss 3.6249 Accuracy 0.5344\n",
      "Epoch 12 Batch 150 Loss 3.6258 Accuracy 0.5336\n",
      "Epoch 12 Batch 200 Loss 3.6265 Accuracy 0.5313\n",
      "Epoch 12 Batch 250 Loss 3.6555 Accuracy 0.5308\n",
      "Epoch 12 Batch 300 Loss 3.6659 Accuracy 0.5311\n",
      "Epoch 12 Batch 350 Loss 3.6730 Accuracy 0.5314\n",
      "Epoch 12 Batch 400 Loss 3.6842 Accuracy 0.5299\n",
      "Epoch 12 Batch 450 Loss 3.6846 Accuracy 0.5291\n",
      "Epoch 12 Batch 500 Loss 3.6742 Accuracy 0.5295\n",
      "Epoch 12 Batch 550 Loss 3.6668 Accuracy 0.5296\n",
      "Epoch 12 Batch 600 Loss 3.6594 Accuracy 0.5300\n",
      "Epoch 12 Batch 650 Loss 3.6523 Accuracy 0.5303\n",
      "Epoch 12 Batch 700 Loss 3.6465 Accuracy 0.5304\n",
      "Epoch 12 Loss 3.6472 Accuracy 0.5303\n",
      "Time taken for 1 epoch: 118.11153244972229 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 3.3857 Accuracy 0.5489\n",
      "Epoch 13 Batch 50 Loss 3.5984 Accuracy 0.5290\n",
      "Epoch 13 Batch 100 Loss 3.5733 Accuracy 0.5311\n",
      "Epoch 13 Batch 150 Loss 3.5535 Accuracy 0.5337\n",
      "Epoch 13 Batch 200 Loss 3.5511 Accuracy 0.5333\n",
      "Epoch 13 Batch 250 Loss 3.5384 Accuracy 0.5335\n",
      "Epoch 13 Batch 300 Loss 3.5196 Accuracy 0.5349\n",
      "Epoch 13 Batch 350 Loss 3.5021 Accuracy 0.5354\n",
      "Epoch 13 Batch 400 Loss 3.4879 Accuracy 0.5353\n",
      "Epoch 13 Batch 450 Loss 3.4746 Accuracy 0.5353\n",
      "Epoch 13 Batch 500 Loss 3.4603 Accuracy 0.5352\n",
      "Epoch 13 Batch 550 Loss 3.4426 Accuracy 0.5359\n",
      "Epoch 13 Batch 600 Loss 3.4259 Accuracy 0.5369\n",
      "Epoch 13 Batch 650 Loss 3.4188 Accuracy 0.5368\n",
      "Epoch 13 Batch 700 Loss 3.4064 Accuracy 0.5375\n",
      "Epoch 13 Loss 3.4060 Accuracy 0.5376\n",
      "Time taken for 1 epoch: 118.11981749534607 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 3.7489 Accuracy 0.4804\n",
      "Epoch 14 Batch 50 Loss 3.2561 Accuracy 0.5440\n",
      "Epoch 14 Batch 100 Loss 3.2585 Accuracy 0.5426\n",
      "Epoch 14 Batch 150 Loss 3.2640 Accuracy 0.5419\n",
      "Epoch 14 Batch 200 Loss 3.2552 Accuracy 0.5431\n",
      "Epoch 14 Batch 250 Loss 3.2480 Accuracy 0.5440\n",
      "Epoch 14 Batch 300 Loss 3.2532 Accuracy 0.5436\n",
      "Epoch 14 Batch 350 Loss 3.2535 Accuracy 0.5438\n",
      "Epoch 14 Batch 400 Loss 3.2328 Accuracy 0.5451\n",
      "Epoch 14 Batch 450 Loss 3.2033 Accuracy 0.5481\n",
      "Epoch 14 Batch 500 Loss 3.2027 Accuracy 0.5482\n",
      "Epoch 14 Batch 550 Loss 3.2042 Accuracy 0.5481\n",
      "Epoch 14 Batch 600 Loss 3.1989 Accuracy 0.5490\n",
      "Epoch 14 Batch 650 Loss 3.1937 Accuracy 0.5496\n",
      "Epoch 14 Batch 700 Loss 3.1722 Accuracy 0.5518\n",
      "Epoch 14 Loss 3.1721 Accuracy 0.5518\n",
      "Time taken for 1 epoch: 118.01613330841064 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 2.6844 Accuracy 0.6208\n",
      "Epoch 15 Batch 50 Loss 2.7678 Accuracy 0.5980\n",
      "Epoch 15 Batch 100 Loss 2.8377 Accuracy 0.5886\n",
      "Epoch 15 Batch 150 Loss 2.8224 Accuracy 0.5914\n",
      "Epoch 15 Batch 200 Loss 2.8283 Accuracy 0.5908\n",
      "Epoch 15 Batch 250 Loss 2.8192 Accuracy 0.5927\n",
      "Epoch 15 Batch 300 Loss 2.8070 Accuracy 0.5943\n",
      "Epoch 15 Batch 350 Loss 2.7993 Accuracy 0.5951\n",
      "Epoch 15 Batch 400 Loss 2.7932 Accuracy 0.5961\n",
      "Epoch 15 Batch 450 Loss 2.7906 Accuracy 0.5965\n",
      "Epoch 15 Batch 500 Loss 2.7913 Accuracy 0.5964\n",
      "Epoch 15 Batch 550 Loss 2.7908 Accuracy 0.5964\n",
      "Epoch 15 Batch 600 Loss 2.7898 Accuracy 0.5965\n",
      "Epoch 15 Batch 650 Loss 2.7825 Accuracy 0.5975\n",
      "Epoch 15 Batch 700 Loss 2.7833 Accuracy 0.5974\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train\\ckpt-3\n",
      "Epoch 15 Loss 2.7835 Accuracy 0.5974\n",
      "Time taken for 1 epoch: 119.35808825492859 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 2.5958 Accuracy 0.6190\n",
      "Epoch 16 Batch 50 Loss 2.6910 Accuracy 0.6069\n",
      "Epoch 16 Batch 100 Loss 2.6959 Accuracy 0.6071\n",
      "Epoch 16 Batch 150 Loss 2.7024 Accuracy 0.6061\n",
      "Epoch 16 Batch 200 Loss 2.7000 Accuracy 0.6063\n",
      "Epoch 16 Batch 250 Loss 2.6998 Accuracy 0.6062\n",
      "Epoch 16 Batch 300 Loss 2.7040 Accuracy 0.6058\n",
      "Epoch 16 Batch 350 Loss 2.7045 Accuracy 0.6057\n",
      "Epoch 16 Batch 400 Loss 2.7033 Accuracy 0.6058\n",
      "Epoch 16 Batch 450 Loss 2.7028 Accuracy 0.6058\n",
      "Epoch 16 Batch 500 Loss 2.7607 Accuracy 0.6006\n",
      "Epoch 16 Batch 550 Loss 2.8206 Accuracy 0.5947\n",
      "Epoch 16 Batch 600 Loss 2.8375 Accuracy 0.5931\n",
      "Epoch 16 Batch 650 Loss 2.8300 Accuracy 0.5938\n",
      "Epoch 16 Batch 700 Loss 2.8257 Accuracy 0.5941\n",
      "Epoch 16 Loss 2.8260 Accuracy 0.5941\n",
      "Time taken for 1 epoch: 118.27076196670532 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 3.0436 Accuracy 0.5594\n",
      "Epoch 17 Batch 50 Loss 2.7263 Accuracy 0.6031\n",
      "Epoch 17 Batch 100 Loss 2.7052 Accuracy 0.6068\n",
      "Epoch 17 Batch 150 Loss 2.7055 Accuracy 0.6066\n",
      "Epoch 17 Batch 200 Loss 2.7117 Accuracy 0.6053\n",
      "Epoch 17 Batch 250 Loss 2.7220 Accuracy 0.6043\n",
      "Epoch 17 Batch 300 Loss 2.7303 Accuracy 0.6034\n",
      "Epoch 17 Batch 350 Loss 2.7389 Accuracy 0.6026\n",
      "Epoch 17 Batch 400 Loss 2.7393 Accuracy 0.6025\n",
      "Epoch 17 Batch 450 Loss 2.7390 Accuracy 0.6023\n",
      "Epoch 17 Batch 500 Loss 2.7412 Accuracy 0.6021\n",
      "Epoch 17 Batch 550 Loss 2.7366 Accuracy 0.6026\n",
      "Epoch 17 Batch 600 Loss 2.7387 Accuracy 0.6023\n",
      "Epoch 17 Batch 650 Loss 2.7419 Accuracy 0.6018\n",
      "Epoch 17 Batch 700 Loss 2.7434 Accuracy 0.6015\n",
      "Epoch 17 Loss 2.7431 Accuracy 0.6016\n",
      "Time taken for 1 epoch: 118.09255576133728 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 2.6216 Accuracy 0.6127\n",
      "Epoch 18 Batch 50 Loss 2.6870 Accuracy 0.6070\n",
      "Epoch 18 Batch 100 Loss 2.7120 Accuracy 0.6036\n",
      "Epoch 18 Batch 150 Loss 2.7070 Accuracy 0.6037\n",
      "Epoch 18 Batch 200 Loss 2.7071 Accuracy 0.6043\n",
      "Epoch 18 Batch 250 Loss 2.7085 Accuracy 0.6043\n",
      "Epoch 18 Batch 300 Loss 2.7154 Accuracy 0.6037\n",
      "Epoch 18 Batch 350 Loss 2.7263 Accuracy 0.6027\n",
      "Epoch 18 Batch 400 Loss 2.7308 Accuracy 0.6023\n",
      "Epoch 18 Batch 450 Loss 2.7320 Accuracy 0.6023\n",
      "Epoch 18 Batch 500 Loss 2.7309 Accuracy 0.6025\n",
      "Epoch 18 Batch 550 Loss 2.7291 Accuracy 0.6029\n",
      "Epoch 18 Batch 600 Loss 2.7361 Accuracy 0.6024\n",
      "Epoch 18 Batch 650 Loss 2.7405 Accuracy 0.6022\n",
      "Epoch 18 Batch 700 Loss 2.7451 Accuracy 0.6020\n",
      "Epoch 18 Loss 2.7452 Accuracy 0.6019\n",
      "Time taken for 1 epoch: 118.14182662963867 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 2.7718 Accuracy 0.6016\n",
      "Epoch 19 Batch 50 Loss 2.8048 Accuracy 0.5984\n",
      "Epoch 19 Batch 100 Loss 2.8095 Accuracy 0.5970\n",
      "Epoch 19 Batch 150 Loss 2.8103 Accuracy 0.5969\n",
      "Epoch 19 Batch 200 Loss 2.7993 Accuracy 0.5985\n",
      "Epoch 19 Batch 250 Loss 2.7918 Accuracy 0.5992\n",
      "Epoch 19 Batch 300 Loss 2.7818 Accuracy 0.6004\n",
      "Epoch 19 Batch 350 Loss 2.7778 Accuracy 0.6008\n",
      "Epoch 19 Batch 400 Loss 2.7783 Accuracy 0.6009\n",
      "Epoch 19 Batch 450 Loss 2.7742 Accuracy 0.6010\n",
      "Epoch 19 Batch 500 Loss 2.7757 Accuracy 0.6007\n",
      "Epoch 19 Batch 550 Loss 2.7784 Accuracy 0.6000\n",
      "Epoch 19 Batch 600 Loss 2.7769 Accuracy 0.5999\n",
      "Epoch 19 Batch 650 Loss 2.7721 Accuracy 0.6003\n",
      "Epoch 19 Batch 700 Loss 2.7701 Accuracy 0.6003\n",
      "Epoch 19 Loss 2.7701 Accuracy 0.6003\n",
      "Time taken for 1 epoch: 118.27622365951538 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 2.7297 Accuracy 0.6066\n",
      "Epoch 20 Batch 50 Loss 2.7321 Accuracy 0.6020\n",
      "Epoch 20 Batch 100 Loss 2.7415 Accuracy 0.6001\n",
      "Epoch 20 Batch 150 Loss 2.7319 Accuracy 0.6018\n",
      "Epoch 20 Batch 200 Loss 2.7245 Accuracy 0.6027\n",
      "Epoch 20 Batch 250 Loss 2.7266 Accuracy 0.6025\n",
      "Epoch 20 Batch 300 Loss 2.7311 Accuracy 0.6025\n",
      "Epoch 20 Batch 350 Loss 2.7370 Accuracy 0.6020\n",
      "Epoch 20 Batch 400 Loss 2.7403 Accuracy 0.6016\n",
      "Epoch 20 Batch 450 Loss 2.7470 Accuracy 0.6007\n",
      "Epoch 20 Batch 500 Loss 2.7514 Accuracy 0.6002\n",
      "Epoch 20 Batch 550 Loss 2.7519 Accuracy 0.6002\n",
      "Epoch 20 Batch 600 Loss 2.7520 Accuracy 0.6003\n",
      "Epoch 20 Batch 650 Loss 2.7524 Accuracy 0.6004\n",
      "Epoch 20 Batch 700 Loss 2.7514 Accuracy 0.6006\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train\\ckpt-4\n",
      "Epoch 20 Loss 2.7515 Accuracy 0.6006\n",
      "Time taken for 1 epoch: 119.26501750946045 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估（Evaluate）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下步骤用于评估：\n",
    "\n",
    "+ 用葡萄牙语分词器（tokenizer_pt）编码输入语句。此外，添加开始和结束标记，这样输入就与模型训练的内容相同。这是编码器输入。\n",
    "+ 解码器输入为 start token == tokenizer_en.vocab_size。\n",
    "+ 计算填充遮挡和前瞻遮挡。\n",
    "+ 解码器通过查看编码器输出和它自身的输出（自注意力）给出预测。\n",
    "+ 选择最后一个词并计算它的 argmax。\n",
    "+ 将预测的词连接到解码器输入，然后传递给解码器。\n",
    "+ 在这种方法中，解码器根据它预测的之前的词预测下一个。\n",
    "\n",
    "\n",
    "Note：这里使用的模型具有较小的能力以保持相对较快，因此预测可能不太正确。要复现论文中的结果，请使用全部数据集，并通过修改上述超参数来使用基础 transformer 模型或者 transformer XL。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "    # 输入语句是葡萄牙语，增加开始和结束标记\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # 因为目标是英语，输入 transformer 的第一个词应该是\n",
    "    # 英语的开始标记。\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    print(\"1\")\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # 从 seq_len 维度选择最后一个词\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "        print(\"2\")\n",
    "        print(predictions)\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        print(\"3\")\n",
    "        print(predicted_id)\n",
    "        # 如果 predicted_id 等于结束标记，就返回结果\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "        print(\"4\")\n",
    "        # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        print(\"5\")\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # 画出注意力权重\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    print(result.numpy())\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "tf.Tensor(\n",
      "[[[ -0.1039823  -2.517206    3.269055  ... -12.327451  -12.299545\n",
      "    11.941761 ]]], shape=(1, 1, 8089), dtype=float32)\n",
      "3\n",
      "tf.Tensor([[8088]], shape=(1, 1), dtype=int32)\n",
      "[8087]\n",
      "Input: este é um problema que temos que resolver.\n",
      "Predicted translation: \n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "tf.Tensor(\n",
      "[[[  0.04906645  -2.3871815    3.5137234  ... -12.109138   -12.086848\n",
      "    11.908133  ]]], shape=(1, 1, 8089), dtype=float32)\n",
      "3\n",
      "tf.Tensor([[8088]], shape=(1, 1), dtype=int32)\n",
      "[8087]\n",
      "Input: os meus vizinhos ouviram sobre esta ideia.\n",
      "Predicted translation: \n",
      "Real translation: and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "tf.Tensor(\n",
      "[[[ -0.7790793  -2.7278092   3.0551472 ... -13.353612  -13.307756\n",
      "    10.714002 ]]], shape=(1, 1, 8089), dtype=float32)\n",
      "3\n",
      "tf.Tensor([[8088]], shape=(1, 1), dtype=int32)\n",
      "[8087]\n",
      "Input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n",
      "Predicted translation: \n",
      "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n"
     ]
    }
   ],
   "source": [
    "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以为 plot 参数传递不同的层和解码器的注意力模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "[8087]\n",
      "Input: este é o primeiro livro que eu fiz.\n",
      "Predicted translation: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Attempting to set identical bottom == top == -0.5 results in singular transformations; automatically expanding.\n",
      "  if sys.path[0] == '':\n",
      "F:\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Attempting to set identical bottom == top == -0.5 results in singular transformations; automatically expanding.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHgAAAEqCAYAAABujOcYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5StdX3f8feXW+Ry0ChHMVI4aPCCFgRZKorxFjS1SRs0LqumESVqtAkaJUQSGxurpo2tXQXrHY9GEwRFUnKBeOOiYjDA4ZwjAjarahujCdoYiKIIfPvH8wxuD2dm9sz8fs9+Lu/XWnvt2c/M+X1/v71nPmf2d55LZCaSJEmSJEkarj0WPQFJkiRJkiRtjA0eSZIkSZKkgbPBI0mSJEmSNHA2eCRJkiRJkgbOBo8kSZIkSdLA2eCRJEmSJEkaOBs8kiRJkiRJA2eDR5IkSZIkaeBs8EiSJEmSJA2cDR5JkiRJkqSB22vRE1A9EbEP8OD24Y2Z+YNFzkfSuJk5krpk5kjqkpmjIYjMXPQcVEFEPAl4P/AVIIB/BrwgMy9f4LQkjZSZI6lLZo6kLpk5GgobPCMVEVcDz8vMG9vHDwbOycxHLXZmksbIzJHUJTNHUpfMHA2F5+AZr72XAgggM78E7L3A+UgaNzNHUpfMHEldMnM0CFUaPNH444h4WI3xNZerI+LsiHhSe3s3cPWiJyXVYOb0gpmjSTF3Fs7M0aSYOQtn5mgQqhyiFRFPB84Gzs3MVxcvoFVFxI8B/w44geY40cuBt2Xm9xc6MakCM2fxzBxNjbmzWGaOpsbMWSwzR0NRq8FzHvBe4EzgyMy8vXgRLSsi9gB2ZOYjFj0XqQtmzmKZOZoic2dxzBxNkZmzOGaOhqT4IVoRcRDw8My8GPgEcFLpGlpZZt4JbI+IQxc9F6k2M2fxzBxNjbmzWGaOpsbMWSwzR0NS4xw8vwSc0368FTilQg2t7v7AdRHxyYi4cOm26Elp2CLipIg4YNHz2IWZ0w9mjqowd7QMM0dVmDlahpmjKkpnTvFDtCJiJ/Azmfm19vF24Gcz8/8WLaQVRcQTd7c9My/rei4ah4h4EHAD8GuZ+Y5Fz2eJmdMPZo5qMHe0HDNHNZg5Wo6ZoxpqZE7RBk9E3At4Tma+c2bbicA3M3NbsUKSOhcRbwQSeFpmPnrR8wEzRxo7c0dSl8wcSV2qkTlFD9HKzG8DX9hl28eB/UrW0fIi4jPt/S0RcfPM7ZaIuHnR89MwRcSewLOB/wz8Y0QcveApAWZOH5g5qsXc0e6YOarFzNHumDmqpVbm1DgHz1lzblMFmXlCe78pMw+cuW3KzAMXPT8N1jOAKzLzFporOPzyguczy8xZIDNHFZk7uhszRxWZObobM0cVVcmcvUoMAhARxwOPAzZHxKtmPnUgsGepOppfRJwAHJGZW9uz72/KzC8vel4apFOA/9p+fAHwhoh4dWbetqgJmTn9Y+aoMHNHKzJzVJiZoxWZOSqsSuaU3INnH+AAmqbRppnbzcAvFKyjOUTE64DfBM5oN+0DfHBxM9JQtcd+3yszPw2Qmd8DPgI8ZaETM3N6xcxRSeaOVmPmqCQzR6sxc1RSzcwpfZLlPYFzM9PAWbCIuBY4BrgmM49pt+3IzKMWOzOpHDOnP8wcTYW50w9mjqbCzOkHM0dDUewQLYDMvCMi7l1yTK3bbZmZEZEAEbH/oiek4YmIY1f6fGZe09Vclqlv5vSHmaMizB3NycxREWaO5mTmqIjamVO0wdPaFhEXAh8GvrO0MTM/WqGWlndeRLwTuFdEvBh4EfDuBc9Jw7N0XOg9gOOA7UAARwFXAicsaF6zzJx+MHNUirmjeZg5KsXM0TzMHJVSNXOKHqIFEBFbd7M5M/NFRQtpVRFxIvA0mm+Yv2gvqSitWUR8CHhjZu5sHz8COC0zT17oxDBz+sTMUUnmjlZj5qgkM0erMXNUUq3MKd7gUb9ExIHM7KmVmf9vgdPRQEXEtZn5yNW2SWaOSjF3NA8zR6WYOZqHmaNSamVO8UO0IuIeNJf8ejjNbkcA2GHuVkS8FHg9cCtwJ02nOYEHLnJeGqzrI+I9NFcLSOAXgesXO6WGmdMPZo4qMHe0LDNHFZg5WpaZowqqZE7Jy6Qv+QBwMPB04DLgEOCWUoNHxP0i4uyIuKh9fGREnFJq/BE5DXh4Zm7JzAdm5uGZWSyAIuKQiLggIm6KiL+LiPMj4pBS46t3XghcB7wCeCXwxXZbH5g5/WDmqLRJ5o6ZM7eqmQPmzgRNMnPA3JmTmaPSqmROjXPwbMvMY5YuGxcRe9Mco7jha7q3418EbAV+OzOPjoi9gG2Z+c9LjD8WEXEx8MzM/G6l8T8O/BHNfzjQdByfn5kn1qgnLcfM6QczR1NSM3fMnPnUzpy2hrmjXvB3ncUzczQUNa6i9YP2/tvtiYK+AWwpOP5BmXleRJwBkJm3R8QdBccfizOAKyLiSuD7Sxsz89RC42/OzNkTvr0vIl5ZaOy7RMRhwBGZ+YmI2BfYKzOL/cVC84mIxwP/ATiMHz3uuA+7pZo5/WDmqKgJ546ZM5/amQMd5I6Z0x8Tzhwwd+YxiswBc6cvamVOjQbPuyLix4HXAhcCBwD/vuD434mI+9Acp0ZEPBb4x4Ljj8U7gU8BO2mOEy3tmxHxi8A57ePnAt8qWSCaSxC+BLg38CCa3VHfATy1ZB3N5Wzg14Grgb79h2/m9IOZo9KmmjtmznxqZw5Uzh0zp3emmjlg7sxj8JkD5k7PVMmcGodoHZ6ZX15t2wbGPxY4C3gE8AVgM/DszNxeYvyxiIgrMvNxFcc/FHgrcDzNfwZXAK/IzK8WrHEt8Gjgysw8pt22091FuxcRV2bmYxY9j90xc/rBzFFpU80dM2c+tTOnrVE1d8ycfplq5rRjmTurGEPmtDXMnZ6olTk19uA5Hzh2l20fAR5VaPzrgCcCD6E5e/mN1DlZ9NBdEhEvAf6EH92NcMOX8ouIPYE3Zea/2uhYq/h+Zt4WEUt196L9y4I6d0lEvBn4KD/6/XTN4qZ0FzOnH8wclTbV3DFz5lMtc6Cz3DFz+mWqmQPmzjzGkDlg7vRJlcwp1uCJiIfSXLrvnhHxzJlPHcjM5fwK+FxmHksTREu1r+HuoTd1z2vvz5jZVuRSfpl5R0Rsjoh9MvO2jY63gssi4reAfSPiRODlNKGq7i11l4+b2ZZAkZP7rYeZ0ztmjkqbau6YOfOpljnQWe6YOf0y1cwBc2ceY8gcMHf6pErmlNyD5yHAzwL3An5uZvstwIs3OnhEHAw8gOab8Ria7jI0AbffRscfm8w8vHKJrwCfjYgLge/M1H1LwRqvAU6hOdb1pcCfA+8pOL7mlJlPXvQcdsPM6REzR6VNLXfMnLXpIHOgfu6YOT0ytcwBc2ctRpI5YO70Rq3MqXEOnuMz83NFB23GfQFwMk2H66/4YQDdArwvMz9auuYQRcRTMvNTu3T571LqeYqI1y0z/u+WGF/9EhH3A94E/ERm/ouIOBI4PjPPXvDUzJwFM3NUy9Ryx8yZT1eZ09YydyZkapnTjmvurMLMUS21MqdGg+f3gTcAtwIXA0cDr8zMDxYa/1mZeX6JscYoIn43M18XEVt38+nMzBcVqnNMZm4rMdYKNb7Mbo4JLXW5yvY52t34RZ6jMYmIi4CtwG9n5tHt8brb+nBCNjNnscycNdcwd+Y01dwxc1bWVea0tarmjpnTL1PNnHZ8c2cZY8qctobvr3qiVubUOMny0zLz9Ig4Cfgb4NnAJUCRAAIOiYgDaTrL76Y5NvQ1mfmxQuMPWhtAewAXZeZ5FUu9JSLuD3wY+FBmXrfaP1iH2eMR70HzvXTvguP/6S7jnwT8bcHxiYgnAFdk5h0z247tyQn71uKgzDwvIs4AyMzbI6IvlxA1cxbIzFkzc2d+U80dM2cFHWYO1M8dM6dfppo5YO4sa2SZA76/6pM6mZOZRW/Ade39u4GfaT/eXnD87e3904ELaTrY15Rex9BvwOUd1DgYOBX4LM1xnK/toOZnKo69B/CpwmN+F7gMuN/MtsF9vwKXAvdZmjvwWOCyRc+rnYuZ04ObmbPu8c2d5dcxydwxc+Z+nqpnTlun09wxcxZ3m2rmzI5l7qz4HI0yc9qavr9awK1W5tTYg+dPIuIGml0IXx4Rm4HvFRx/6djQZwBbM3N7RMRK/2CiPh4RpwHn8qMn6SpyKb92rG8AZ0bEJcDpwO/Q7D5aRETMnrl/D5qO86ZS4+/GEcChhce8EXgzcGlEnJKZV/DD7+EheRXNf/gPiojPApuBX1jslO5i5vSDmbM+5s7yppo7Zs58qmdOO1613DFzemeqmQPmzjwGnzng+6ueqZI5xc/BAxARPw7cnM3l3vYHNrXfrCXG3kpztvfDabrLewKXZuajSow/Fu3xlbvKLHd85cOA59B8E34L+BBwfmb+fYnx2xqXzDy8nebM8v8lM28sNP4tNMeIRnv/DeCMLHgMckRck5nHRsQRNP8hvBd4UTaXohyU9rjQh9A8Xzdm5g8WPKW7mDmLZ+bMXcPcWYMp5o6ZM5/amdPWqJo7Zk7/TDFz2rHNnVWMIXPaGr6/6pEamVO0wRMR+wFHZOb2mW2HAndk5tcK1dgDeCTwvzPz2xFxH+ABmbmjxPiaT0T8JXAO8OHMLHpc5ZhExLbMPKb9eH+aE2k9MzNr7D1XRRc/1+tl5kyHmTM/c6eu2nMzc/rD3JmPmVOXv+tMh5kzHzNnlbELN3j2Bm4AjsrM77TbPgb8VmZeVahGAM8HHpiZr2+fiIMz8/NDGH+mztHAE9qHn559cQuNfw/g5cAJNN3TTwPvyMySu3NWFRGvWunzmfmWDY6/9Fofnpn/sdZrvZu6h2bm/6lZo6Qufq77PLcuMqGjGmbOKmpnTlvD3JnDlHNnLJnT1qmWO2bO3DXMnDlMOXPa8Xx/tfrYg88c8P1VX9T8ud6jwPzu0u5SdAHNrmVLXajNhYPxbcDxwHPbx7cA/2MjA0bE4yNiz1rj76beK4A/BO7b3j4YEb9WsgbwB8DDgbOAtwJHAh/Y6KARcV57vzMidszcdkZE6S7/ccDLaHYZfQDwKzTr2ESZY0WXXuvntY+LvdYRcXp7f1ZEnDl7A04rUaMrHf1cr8tQMwe6zR0zZ261MwfMnblMPHcGnzltvdq5UyVzoNPcMXN6YuKZA76/mscYMgd8f9ULVX+us/zZoB9K0zEFeC1wauHxl84yvW1m24bOIg88DnhXrfF3U28HsP/M4/2BHYVr3G3OJdYB3L+9P2x3t8Jr+BjN8cVLjzcBF/f5e2lmnG+1968EXrDrreTz1MWt9s91n+dW6/uky9wxc+auVTVzar/W5s445jaGzGnHq5o7tTKnHaeT3DFz+nWbaubU+j4xc9Y09mh+1zFz1rSeKj/XxY9Ty8wbIoKIeDBNl/aEwiV+0HaDEyCas8jfuZEBM/OKiPhurfF3I4DZa9zf0W4raVtEPDYz/xIgIh5Dc7m9DcnMr7f3X93oWHM4FLht5vFtwJaC49d8rf8uIg4DXgg8udCYC9PBz/W6DTFzoPPcMXPmUztzwNyZ24RzZwyZA/Vzp0rmQKe5Y+b0yIQzB3x/NY8xZA74/qo3av1c1zoR0dnAe2i6pv9QeOwzaXZnum9EvJHmLOOv3eigmXltzfF3sRW4MiIuaB//PM1zVtJjgF+KiKVjEQ8Fro+InTRnfD9qPYPGD8+MfrdPteMeuK7Z7t4HgM+3z1MCJwHvLzh+zdf67cDFwAOB2V3tls4oX+yM+8uJiIOz0NUVWjV/rjdqcJkDneaOmTOf2pkDI86dCpkD08ydMWQO1M+dKpkDneaOmbMBZk5Rvr9a3RgyB3x/tSFDeH9V6zLp+wFfB56VmZ+oMP5DgafSvJifzMzrhzR+W+NYmi5dAJdn5rbC4x+20uc76hBvWPs8LZ0srcbzVPt76e2Z+bKSY66h9p9l5r8sOF7Vn+uNGHrmdFHDzJlP7cxpa4wyd0pnTjvmJHNnDJnT1qiWO2bOmmqYOfOPOcnMacf3/dXKY48ic8D3Vxus3fv3V1UaPJIkSZIkSepO0atoSZIkSZIkqXtVGzwR8ZKa43dRYwxr6KKGa5hOjS7WsF5jeH67qOEaplNjDGvoqsZ6+Rr2o4ZrmE6NMaxhI8awftcwnRpjWEMXNUqOX3sPni7CsXaNMayhixquYTo1evtLD+N4fruo4RqmU2MMa+iqxnr5GvajhmuYTo0xrGEjxrB+1zCdGmNYQxc1BtPgkSRJkiRJUmVrOsnyQQcdlFu2bJn762+66SY2b968jmnNr3aNMayhixquYTo11jP+1Vdf/c3MXPOkppg5XdRwDdOpMYY1rKdGV5kDvoZ9qeEaplOjj2tYb+bANH/XcQ3TqTGGNXRRo+T7q73WMsiWLVu46qqrVv9CSZoREeu6dKSZI2k9zBxJXVpv5oC5I2l9lsudNe3BExE3AWsJsIOAb67h69ejdo0xrKGLGq5hOjXWM/5h6/nL1kQzp4sarmE6NcawhvXU6CpzwNewLzVcw3Rq9HEN68ocmOzvOq5hOjXGsIYuahR7f7WmBs9aRcRVmXlctQId1BjDGrqo4RqmU6OLNazXGJ7fLmq4hunUGMMauqqxXr6G/ajhGqZTYwxr2IgxrN81TKfGGNbQRY2S43uSZUmSJEmSpIGzwSNJkiRJkjRwtRs876o8fhc1xrCGFWtExD/t8vjkiHhrifEj4tKIuNvuZhHxqxHx1xGREXHQRmoUNPrXeiDjb8QYnt8uaix8DQVyZ6VMWy53/jAiboyIL0TEeyNi7/XWKGj0r/WAaqyXr2E/atTOnGVrrJA5Z0fE9ojYEREfiYgD1jN+YWOoMYY1bMQY1j/6NSwic2Y+f9au9ddao6AxvNZd1Cg2ftVz8GgYIuKfMvOAmccnA8dl5q8WGPtS4LTMvGqX7ccA/wBc2taqfWIsST2yoNx5BnBR+/CPgMsz8+0brSep/xaUOQdm5s3tx28B/j4z/9NG60nqv0VkTvu544BXACfN1td0eIiWVhQRmyPi/Ij4q/b2+Hb7oyPiiojY1t4/pN2+b0R8qP1r1bnAvrsbNzO3ZeZXuluJpKGomDt/ni3g88AhnS1KUm9VzJyl5k60X+NfVSVVy5yI2BN4M3B6Z4tR7+y16AmoF/aNiGtnHt8buLD9+L8D/y0zPxMRhwJ/ATwMuAH4qcy8PSJ+GngT8CzgZcB3M/OoiDgKuKazVUgakoXlTnto1r+l+QuXpGlYSOZExFbgGcAXgVeXXpSk3lpE5vwqcGFmfr3pK2uKbPAI4NbMfOTSg6VdCNuHPw0cORMSB0bEJuCewPsj4giav0gtncvip4AzATJzR0TsqD99SQO0yNx5G83hWZ8usRBJg7CQzMnMF7Z/VT8LeA6wtdiKJPVZp5kTET8BPBt4UvGVaFBs8Gg1ewDHZ+atsxsj4izgksw8KSK20JxLZ4m7IEvaiGq5ExGvAzYDLy0yU0ljUPV3ncy8oz2s4jewwSOpTuYcA/wk8Ndt42i/iPjrzPzJUpPWMHgOHq3mYzS7+wEQEUud6HsCX2s/Pnnm6y8Hnt9+7SOAo+pPUdLIVMmdiPhl4OnAczPzzrJTljRgxTMnGj+59DHwczSHX0hS8czJzD/LzIMzc0tmbqE5pMvmzgTZ4NFqTgWOa0/q9UXgV9rtvw/8XkR8Fthz5uvfDhzQ7jp4Os2JTO8mIk6NiL+hOcnpjoh4T7UVSBqaKrkDvAO4H/C5iLg2In6nzvQlDUyNzAmaQy12AjuB+wOvr7UASYNS6/ccycukS5IkSZIkDZ178EiSJEmSJA2cDR5JkiRJkqSBs8EjSZIkSZI0cDZ4JEmSJEmSBs4GjyRJkiRJ0sDZ4JEkSZIkSRo4GzySJEmSJEkDZ4NHkiRJkiRp4GzwSJIkSZIkDZwNHkmSJEmSpIGzwSNJkiRJkjRwNngkSZIkSZIGzgaPJEmSJEnSwNngkSRJkiRJGjgbPJIkSZIkSQNng0eSJEmSJGngbPBIkiRJkiQNnA0eSZIkSZKkgbPBI0mSJEmSNHA2eCRJkiRJkgbOBo8kSZIkSdLA2eCRJEmSJEkaOBs8kiRJkiRJA2eDR5IkSZIkaeBs8EiSJEmSJA2cDR5JkiRJkqSBs8EjSZIkSZI0cDZ4JEmSJEmSBs4GjyRJkiRJ0sDZ4JEkSZIkSRo4GzySJEmSJEkDZ4NHkiRJkiRp4GzwSJIkSZIkDZwNHkmSJEmSpIGzwSNJkiRJkjRwNngkSZIkSZIGzgaPJEmSJEnSwNngkSRJkiRJGjgbPJIkSZIkSQNng0eSJEmSJGngbPBIkiRJkiQNnA0eSZIkSZKkgbPBI0mSJEmSNHA2eCRJkiRJkgbOBo8kSZIkSdLA2eCRJEmSJEkaOBs8kiRJkiRJA2eDR5IkSZIkaeBs8EiSJEmSJA2cDR5JkiRJkqSBs8EjSZIkSZI0cDZ4JEmSJEmSBs4GjyRJkiRJ0sDZ4JEkSZIkSRo4GzySJEmSJEkDZ4NHkiRJkiRp4GzwSJIkSZIkDZwNHkmSJEmSpIGzwSNJkiRJkjRwNngkSZIkSZIGzgaPJEmSJEnSwNngkSRJkiRJGjgbPJIkSZIkSQNng0eSJEmSJGngbPBIkiRJkiQNnA0eSZIkSZKkgbPBI0mSJEmSNHA2eCRJkiRJkgbOBo8kSZIkSdLA2eCRJEmSJEkaOBs8kiRJkiRJA2eDR5IkSZIkaeBs8EiSJEmSJA2cDR5JkiRJkqSBs8EjSZIkSZI0cDZ4JEmSJEmSBs4GjyRJkiRJ0sDZ4JEkSZIkSRo4GzySJEmSJEkDZ4NHkiRJkiRp4GzwSJIkSZIkDZwNHkmSJEmSpIGzwSNJkiRJkjRwNngkSZIkSZIGzgaPJEmSJEnSwNngkSRJkiRJGjgbPJIkSZIkSQNng0eSJEmSJGngbPBIkiRJkiQNnA0eSZIkSZKkgbPBI0mSJEmSNHA2eCRJkiRJkgbOBo8kSZIkSdLA2eCRJEmSJEkaOBs8kiRJkiRJA2eDR5IkSZIkaeBs8EiSJEmSJA2cDR5JkiRJkqSBs8EjSZIkSZI0cDZ4JEmSJEmSBs4GjyRJkiRJ0sDZ4JEkSZIkSRo4GzySJEmSJEkDZ4NHkiRJkiRp4GzwSJIkSZIkDZwNHkmSJEmSpIGzwSNJkiRJkjRwNngkSZIkSZIGzgaPJEmSJEnSwNngkSRJkiRJGjgbPJIkSZIkSQNng0eSJEmSJGngbPBIkiRJkiQNnA0eSZIkSZKkgbPBI0mSJEmSNHA2eCRJkiRJkgbOBo8kSZIkSdLA2eCRJEmSJEkaOBs8kiRJkiRJA2eDR5IkSZIkaeBs8EiSJEmSJA2cDR5JkiRJkqSBs8EjSZIkSZI0cDZ4JEmSJEmSBs4GjyRJkiRJ0sDZ4JEkSZIkSRo4GzySJEmSJEkDZ4NHkiRJkiRp4GzwSJIkSZIkDZwNHkmSJEmSpIGzwSNJkiRJkjRwNngkSZIkSZIGzgaPJEmSJEnSwNngkSRJkiRJGjgbPJIkSZIkSQNng0eSJEmSJGngbPBIkiRJkiQNnA0eSZIkSZKkgbPBI0mSJEmSNHA2eCRJkiRJkgbOBo8kSZIkSdLA2eCRJEmSJEkaOBs8kiRJkiRJA2eDR5IkSZIkaeBs8EiSJEmSJA2cDR5JkiRJkqSBs8EjSZIkSZI0cDZ4JEmSJEmSBs4GjyRJkiRJ0sDZ4JEkSZIkSRo4GzySJEmSJEkDZ4NHkiRJkiRp4GzwSJIkSZIkDZwNHkmSJEmSpIGzwSNJkiRJkjRwey16AqonIvYBHtw+vDEzf7DI+UgaNzNHUpfMHEldMnM0BJGZi56DKoiIJwHvB74CBPDPgBdk5uULnJakkTJzJHXJzJHUJTNHQ2GDZ6Qi4mrgeZl5Y/v4wcA5mfmoxc5M0hiZOZK6ZOZI6pKZo6HwHDzjtfdSAAFk5peAvRc4H0njZuZI6pKZI6lLZo4GoUqDJxp/HBEPqzG+5nJ1RJwdEU9qb+8Grl70pKQazJxeMHM0KebOwpk5mhQzZ+HMHA1ClUO0IuLpwNnAuZn56uIFtKqI+DHg3wEn0Bwnejnwtsz8/kInJlVg5iyemaOpMXcWy8zR1Jg5i2XmaChqNXjOA94LnAkcmZm3Fy+iZUXEHsCOzHzEoucidcHMWSwzR1Nk7iyOmaMpMnMWx8zRkBQ/RCsiDgIenpkXA58ATipdQyvLzDuB7RFx6KLnItVm5iyemaOpMXcWy8zR1Jg5i2XmaEhqnIPnl4Bz2o+3AqdUqKHV3R+4LiI+GREXLt0WPSkNW0ScFBEHLHoeuzBz+sHMURXmjpZh5qgKM0fLMHNURenMKX6IVkTsBH4mM7/WPt4O/Gxm/t+ihbSiiHji7rZn5mVdz0XjEBEPAm4Afi0z37Ho+Swxc/rBzFEN5o6WY+aoBjNHyzFzVEONzCna4ImIewHPycx3zmw7EfhmZm4rVkhS5yLijUACT8vMRy96PmDmSGNn7kjqkpkjqUs1MqfoIVqZ+W3gC7ts+ziwX8k6Wl5EfKa9vyUibp653RIRNy96fhqmiNgTeDbwn4F/jIijFzwlwMzpAzNHtZg72h0zR7WYOdodM0e11MqcGufgOWvObaogM09o7zdl5oEzt02ZeeCi56fBegZwRWbeQnMFh19e8HxmmTkLZOaoInNHd2PmqCIzR3dj5qiiKpmzV4lBACLieOBxwOaIeNXMpw4E9ixVR/OLiBOAIzJza3v2/U2Z+eVFz0uDdArwX9uPLwDeEBGvzszbFjUhM6d/zBwVZu5oRWaOCjNztCIzR4VVyZySe/DsAxxA0zTaNHO7GfiFgnU0h4h4HfCbwBntpn2ADy5uRhqq9tjve2XmpwEy83vAR4CnLHRiZk6vmDkqydzRaswclWTmaDVmjkqqmTmlT7K8J3BuZho4CxYR1wLHANdk5jHtth2ZedRiZyaVY+b0h5mjqTB3+sHM0VSYOf1g5mgoih2iBZCZd0TEvUuOqXW7LTMzIhIgIvZf9IQ0PBFx7Eqfz8xruprLMvXNnP4wc1SEuaM5mSmawW0AABJYSURBVDkqwszRnMwcFVE7c4o2eFrbIuJC4MPAd5Y2ZuZHK9TS8s6LiHcC94qIFwMvAt694DlpeJaOC70HcBywHQjgKOBK4IQFzWuWmdMPZo5KMXc0DzNHpZg5moeZo1KqZk7RQ7QAImLrbjZnZr6oaCGtKiJOBJ5G8w3zF+0lFaU1i4gPAW/MzJ3t40cAp2XmyQudGGZOn5g5Ksnc0WrMHJVk5mg1Zo5KqpU5xRs86peIOJCZPbUy8/8tcDoaqIi4NjMfudo2ycxRKeaO5mHmqBQzR/Mwc1RKrcwpfohWRNyD5pJfD6fZ7QgAO8zdioiXAq8HbgXupOk0J/DARc5Lg3V9RLyH5moBCfwicP1ip9Qwc/rBzFEF5o6WZeaoAjNHyzJzVEGVzCl5mfQlHwAOBp4OXAYcAtxSavCIuF9EnB0RF7WPj4yIU0qNPyKnAQ/PzC2Z+cDMPDwziwVQRBwSERdExE0R8XcRcX5EHFJqfPXOC4HrgFcArwS+2G7rAzOnH8wclTbJ3DFz5lY1c8DcmaBJZg6YO3Myc1RalcypcQ6ebZl5zNJl4yJib5pjFDd8Tfd2/IuArcBvZ+bREbEXsC0z/3mJ8cciIi4GnpmZ3600/seBP6L5DweajuPzM/PEGvWk5Zg5/WDmaEpq5o6ZM5/amdPWMHfUC/6us3hmjoaixlW0ftDef7s9UdA3gC0Fxz8oM8+LiDMAMvP2iLij4PhjcQZwRURcCXx/aWNmnlpo/M2ZOXvCt/dFxCsLjX2XiDgMOCIzPxER+wJ7ZWaxv1hoPhHxeOA/AIfxo8cd92G3VDOnH8wcFTXh3DFz5lM7c6CD3DFz+mPCmQPmzjxGkTlg7vRFrcyp0eB5V0T8OPBa4ELgAODfFxz/OxFxH5rj1IiIxwL/WHD8sXgn8ClgJ81xoqV9MyJ+ETinffxc4FslC0RzCcKXAPcGHkSzO+o7gKeWrKO5nA38OnA10Lf/8M2cfjBzVNpUc8fMmU/tzIHKuWPm9M5UMwfMnXkMPnPA3OmZKplT4xCtwzPzy6tt28D4xwJnAY8AvgBsBp6dmdtLjD8WEXFFZj6u4viHAm8Fjqf5z+AK4BWZ+dWCNa4FHg1cmZnHtNt2urto9yLiysx8zKLnsTtmTj+YOSptqrlj5syndua0NarmjpnTL1PNnHYsc2cVY8ictoa50xO1MqfGHjznA8fusu0jwKMKjX8d8ETgITRnL7+ROieLHrpLIuIlwJ/wo7sRbvhSfhGxJ/CmzPxXGx1rFd/PzNsiYqnuXrR/WVDnLomINwMf5Ue/n65Z3JTuYub0g5mj0qaaO2bOfKplDnSWO2ZOv0w1c8DcmccYMgfMnT6pkjnFGjwR8VCaS/fdMyKeOfOpA5m5nF8Bn8vMY2mCaKn2Ndw99Kbuee39GTPbilzKLzPviIjNEbFPZt620fFWcFlE/Bawb0ScCLycJlTVvaXu8nEz2xIocnK/9TBzesfMUWlTzR0zZz7VMgc6yx0zp1+mmjlg7sxjDJkD5k6fVMmcknvwPAT4WeBewM/NbL8FePFGB4+Ig4EH0HwzHkPTXYYm4Pbb6Phjk5mHVy7xFeCzEXEh8J2Zum8pWOM1wCk0x7q+FPhz4D0Fx9ecMvPJi57Dbpg5PWLmqLSp5Y6ZszYdZA7Uzx0zp0emljlg7qzFSDIHzJ3eqJU5Nc7Bc3xmfq7ooM24LwBOpulw/RU/DKBbgPdl5kdL1xyiiHhKZn5qly7/XUo9TxHxumXG/90S46tfIuJ+wJuAn8jMfxERRwLHZ+bZC56ambNgZo5qmVrumDnz6Spz2lrmzoRMLXPacc2dVZg5qqVW5tRo8Pw+8AbgVuBi4GjglZn5wULjPyszzy8x1hhFxO9m5usiYutuPp2Z+aJCdY7JzG0lxlqhxpfZzTGhpS5X2T5Huxu/yHM0JhFxEbAV+O3MPLo9XndbH07IZuYslpmz5hrmzpymmjtmzsq6ypy2VtXcMXP6ZaqZ045v7ixjTJnT1vD9VU/UypwaJ1l+WmaeHhEnAX8DPBu4BCgSQMAhEXEgTWf53TTHhr4mMz9WaPxBawNoD+CizDyvYqm3RMT9gQ8DH8rM61b7B+swezziPWi+l+5dcPw/3WX8k4C/LTg+EfEE4IrMvGNm27E9OWHfWhyUmedFxBkAmXl7RPTlEqJmzgKZOWtm7sxvqrlj5qygw8yB+rlj5vTLVDMHzJ1ljSxzwPdXfVInczKz6A24rr1/N/Az7cfbC46/vb1/OnAhTQf7mtLrGPoNuLyDGgcDpwKfpTmO87Ud1PxMxbH3AD5VeMzvApcB95vZNrjvV+BS4D5LcwceC1y26Hm1czFzenAzc9Y9vrmz/DommTtmztzPU/XMaet0mjtmzuJuU82c2bHMnRWfo1FmTlvT91cLuNXKnBp78PxJRNxAswvhyyNiM/C9guMvHRv6DGBrZm6PiFjpH0zUxyPiNOBcfvQkXUUu5deO9Q3gzIi4BDgd+B2a3UeLiIjZM/fvQdNx3lRq/N04Aji08Jg3Am8GLo2IUzLzCn74PTwkr6L5D/9BEfFZYDPwC4ud0l3MnH4wc9bH3FneVHPHzJlP9cxpx6uWO2ZO70w1c8DcmcfgMwd8f9UzVTKn+Dl4ACLix4Gbs7nc2/7ApvabtcTYW2nO9n44TXd5T+DSzHxUifHHoj2+cleZ5Y6vfBjwHJpvwm8BHwLOz8y/LzF+W+OSmYe305xZ/r9k5o2Fxr+F5hjRaO+/AZyRBY9BjohrMvPYiDiC5j+E9wIvyuZSlIPSHhf6EJrn68bM/MGCp3QXM2fxzJy5a5g7azDF3DFz5lM7c9oaVXPHzOmfKWZOO7a5s4oxZE5bw/dXPVIjc4o2eCJiP+CIzNw+s+1Q4I7M/FqhGnsAjwT+d2Z+OyLuAzwgM3eUGF/ziYi/BM4BPpyZRY+rHJOI2JaZx7Qf709zIq1nZmaNveeq6OLner3MnOkwc+Zn7tRVe25mTn+YO/Mxc+ryd53pMHPmY+asMnbhBs/ewA3AUZn5nXbbx4DfysyrCtUI4PnAAzPz9e0TcXBmfn4I48/UORp4Qvvw07MvbqHx7wG8HDiBpnv6aeAdmVlyd86qIuJVK30+M9+ywfGXXuvDM/M/1nqtd1P30Mz8PzVrlNTFz3Wf59ZFJnRUw8xZRe3MaWuYO3OYcu6MJXPaOtVyx8yZu4aZM4cpZ047nu+vVh978JkDvr/qi5o/13sUmN9d2l2KLqDZtWypC7W5cDC+DTgeeG77+Bbgf2xkwIh4fETsWWv83dR7BfCHwH3b2wcj4tdK1gD+AHg4cBbwVuBI4AMbHTQizmvvd0bEjpnbzogo3eU/DngZzS6jDwB+hWYdmyhzrOjSa/289nGx1zoiTm/vz4qIM2dvwGklanSlo5/rdRlq5kC3uWPmzK125oC5M5eJ587gM6etVzt3qmQOdJo7Zk5PTDxzwPdX8xhD5oDvr3qh6s91lj8b9ENpOqYArwVOLTz+0lmmt81s29BZ5IHHAe+qNf5u6u0A9p95vD+wo3CNu825xDqA+7f3h+3uVngNH6M5vnjp8Sbg4j5/L82M8632/pXAC3a9lXyeurjV/rnu89xqfZ90mTtmzty1qmZO7dfa3BnH3MaQOe14VXOnVua043SSO2ZOv25TzZxa3ydmzprGHs3vOmbOmtZT5ee6+HFqmXlDRBARD6bp0p5QuMQP2m5wAkRzFvk7NzJgZl4REd+tNf5uBDB7jfs72m0lbYuIx2bmXwJExGNoLre3IZn59fb+qxsdaw6HArfNPL4N2FJw/Jqv9d9FxGHAC4EnFxpzYTr4uV63IWYOdJ47Zs58amcOmDtzm3DujCFzoH7uVMkc6DR3zJwemXDmgO+v5jGGzAHfX/VGrZ/rWiciOht4D03X9B8Kj30mze5M942IN9KcZfy1Gx00M6+tOf4utgJXRsQF7eOfp3nOSnoM8EsRsXQs4qHA9RGxk+aM70etZ9D44ZnR7/apdtwD1zXb3fsA8Pn2eUrgJOD9Bcev+Vq/HbgYeCAwu6vd0hnli51xfzkRcXAWurpCq+bP9UYNLnOg09wxc+ZTO3NgxLlTIXNgmrkzhsyB+rlTJXOg09wxczbAzCnK91erG0PmgO+vNmQI769qXSZ9P+DrwLMy8xMVxn8o8FSaF/OTmXn9kMZvaxxL06UL4PLM3FZ4/MNW+nxHHeINa5+npZOl1Xiean8vvT0zX1ZyzDXU/rPM/JcFx6v6c70RQ8+cLmqYOfOpnTltjVHmTunMacecZO6MIXPaGtVyx8xZUw0zZ/4xJ5k57fi+v1p57FFkDvj+aoO1e//+qkqDR5IkSZIkSd0pehUtSZIkSZIkdc8GjyRJkiRJ0sBVbfBExEtqjt9FjTGsoYsarmE6NbpYw3qN4fntooZrmE6NMayhqxrr5WvYjxquYTo1xrCGjRjD+l3DdGqMYQ1d1Cg5fu09eLoIx9o1xrCGLmq4hunU6O0vPYzj+e2ihmuYTo0xrKGrGuvla9iPGq5hOjXGsIaNGMP6XcN0aoxhDV3UGEyDR5IkSZIkSZWt6SpaBx10UG7ZsmXur7/pppvYvHnzOqY1v9o1xrCGLmq4hunUWM/4V1999Tczc82TmmLmdFHDNUynxhjWsJ4aXWUO+Br2pYZrmE6NPq5hvZkD0/xdxzVMp8YY1tBFjZLvr9bU4ImIm4CvrqHuQcA31/D161G7xhjW0EUN1zCdGusZ/7D1/OIz0czpooZrmE6NMaxhPTW6yhzwNexLDdcwnRp9XMO6Mgcm+7uOa5hOjTGsoYsaxd5franBs1YRcVVmHletQAc1xrCGLmq4hunU6GIN6zWG57eLGq5hOjXGsIauaqyXr2E/ariG6dQYwxo2Ygzrdw3TqTGGNXRRo+T4noNHkiRJkiRp4GzwSJIkSZIkDVztBs+7Ko/fRY0xrGHFGhHxT7s8Pjki3lpi/Ii4NCLutrtZRLwvIr4cEde2t0eut0ZBo3+tBzL+Rozh+e2ixsLXUCB3Vsq05XInIuKNEfGliLg+Ik5db42CRv9aD6jGevka9qNG7cxZtsYKmfPpmd9z/jYi/ng94xc2hhpjWMNGjGH9o1/DgjLnqRFxTZs5n4mIn1xvjYLG8Fp3UaPY+FXPwaNhiIh/yswDZh6fDByXmb9aYOxLgdMy86pdtr8P+NPM/MhGa0gangXlzguBJwMnZ+adEXHfzPz7jdaT1H+LyJxdvuZ84H9m5h9stJ6k/lvQ7zlfAv51Zl4fES8HHp2ZJ2+0nobFQ7S0oojYHBHnR8RftbfHt9sfHRFXRMS29v4h7fZ9I+JDEbEjIs4F9l3oAiQNTsXceRnw+sy8E8DmjiSo/7tORGwCngKstgePpAmomDkJHNh+fE/gb6svRr2z16InoF7YNyKunXl8b+DC9uP/Dvy3zPxMRBwK/AXwMOAG4Kcy8/aI+GngTcCzaN5AfTczj4qIo4BrVqj7xoj4HeCTwGsy8/tllyWpxxaROw8CnhMRJwE3Aadm5v8qvjJJfbSo33UATgI+mZk3F1yPpH5bROb8MvDnEXErcDPw2OKrUu/Z4BHArZl51zlwlnYhbB/+NHBkRCx9+sD2L1H3BN4fEUfQdIv3bj//U8CZAJm5IyJ2LFPzDOAbwD40xxz+JvD6UguS1HuLyJ0fA76XmcdFxDOB9wJPKLckST22iMxZ8lzgPSUWIWkwFpE5vw48IzOvjIjfAN5C0/TRhNjg0Wr2AI7PzFtnN0bEWcAlmXlSRGwBLp359KondsrMr7cffj8itgKnFZmtpDGokjvA3wDntx9fAGzd8EwljUGtzCEi7gM8mmYvHkmCCpkTEZuBozPzynbTucDFpSas4fAcPFrNx4C7TgYWP7za1T2Br7Ufnzzz9ZcDz2+/9hHAUbsbNCLu394H8PPAF0pOWtKgVckdmvNfPKX9+InAl8pMV9LA1cocgGfTXFTie6UmK2nwamTOPwD3jIgHt49PBK4vN2UNhQ0ereZU4Lj2pF5fBH6l3f77wO9FxGeBPWe+/u3AAe2ug6cDn19m3D+MiJ3ATuAg4A1VZi9piGrlzn8CntVmz+/hbsuSGrUyB+DfAOdUmLOk4SqeOZl5O/Bi4PyI2A78W+A3Kq5BPeVl0iVJkiRJkgbOPXgkSZIkSZIGzgaPJEmSJEnSwNngkSRJkiRJGjgbPJIkSZIkSQNng0eSJEmSJGngbPBIkiRJkiQNnA0eSZIkSZKkgbPBI0mSJEmSNHD/H74A/OLaKaqZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real translation: this is the first book i've ever done.\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
